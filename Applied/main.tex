\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{hw}
\usepackage{tcolorbox}
\usepackage[ruled,vlined]{algorithm2e}

% ------------------  Bibliography  ------------------
%\usepackage[
%  backend=biber,
%  style=numeric,      % plain numbers: [1]
%  sorting=none,       % keep refs in citation order
%  maxbibnames=99      % show all authors in bib
%]{biblatex}

% ----------------------------------------------------

\tcbuselibrary{breakable}
\title{305a}
\newcommand{\answer}[1]{%
  \begin{tcolorbox}[
    colback=gray!9.8,
    boxrule=0.5pt,
    breakable]
  \small #1
  \end{tcolorbox}}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\newcommand{\simiid}{\overset{iid}\sim }
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\row}{\operatorname{row}}
\newcommand{\Proj}{\operatorname{Proj}}

\begin{document}
\maketitle
\section{Basic testing}
\begin{enumerate}
    \item Paired t-test
Observations $A_i, B_i$ paired, then $Z_i=A_i - B_i \simiid N(\mu, \sigma^2)$, test $\calH_0 : \mu=0$. 

$$t = \frac{\bar z}{s_z/\sqrt{n}} \sim t_{n-1} \quad \text {under null} \quad \text {where } \quad s_z^2 = \frac{1}{n-1} \sum_{i=1}^n (z_i- \bar z)^2.$$

\item Non parametric test, eg sign test, permutation test


$$\calH_0 \text{ is symmetric about } 0.$$

\item Unpaired 
$$t = \frac{\bar x - \bar y}{s \sqrt{1/n_1 + 1/n_2}} \quad \text {where } s^2 = \frac 1 {n_1 + n_2 -2} \sum_{i=1}^n (x_i - \bar x)^2  + (y_i -\bar y)^2 $$

\end{enumerate}

\section{Least Squares Basics}

\subsection{Basic assumptions and what goes wrong}
If suspect another feature may be important that's not included, can introduce bias.
\begin{enumerate}
	\item Plot residual versus new feature $\omega$
	\item Plot residual against the residual of $\omega \sim X$, ie predict the new feature with existing features (added variable plot)
	\item Add to regression and test goodness of fit 
\end{enumerate}
Another tool is \textbf{log-transforming}. 

\subsection{Heteroskedasticity}
 If we use OLS instead, we get the wrong variance estimate, but our estimate is still unbiased. Will generally make the CI too narrow if we use homoskedastic errors.\\\\
Detection:
\begin{enumerate}
	\item Plot $\hat \eps_i$ versus $X_i$ if a single predictor. If structure, bad.
	\item Plot $\hat \eps_i$ versus $\hat \eps_{i-1}$ to check for correlation
\end{enumerate}
See generalized least squares solution.
\subsection{Non-normality}
For mild non-normality, usually CLT saves the day. See Art section 16.3 for requirements.\\\\
To detect, use QQ plot: plot order statistics $\hat \epsilon_{(i)}$ versus normal quantiles. 
\subsection{Outliers}
Outliers -- violate normality assumption if really far. 	\\ \\

Detection, we might naively look at $|\hat \epsilon_i$ but this isn't great since all of our $\hat \epsilon_i$ are deflated in presence of outlier. Better idea is finding large $\frac{\hat \epsilon_{(-i)}}{s_i}$ where $s_i$ is standard deviation with the $i$-th left out. But issues if more than one outlier or computational issues. \\ \\ 
See "masking", "swamping" outlier issues.\\ \\ 

Another potential idea is 'least trimmed means' which only minimizes residuals of the best 80$\%$ of residuals -- non convex. 

\subsection{LS Estimator derivation}
Least squares derivation:

\begin{align}
    \hat \beta &= \arg \min \|y - X\beta \|^2  \\
    & \iff X\hat \beta = \operatorname{Proj}_{\operatorname{range}(X)}(y)\\
    & \iff y - X \hat \beta \perp \operatorname{range}(X)\\
    &\iff \langle Xv, y - \hat y\rangle =0 \quad \forall v\\
    & \iff X^T (y-\hat y) = 0\\
    & \iff X^TX \hat \beta = X^T y.
\end{align}

If $X$ is not full rank, then $\hat \beta$ is not necessarily unique, but $\hat y$ is. \\
Unbiased:

$$\E \hat \beta = \E (X^TX)^{-1} X^T y = \beta$$

Covariance:

$$\Cov \hat \beta = \sigma^2 (X^T X)^{-1} X^T X (X^TX)^{-1} = \sigma^2 (X^TX)^{-1} $$



\subsection{Hat Matrix}
Facts:
\begin{enumerate}
	\item $\range X = \range H$
	\item $\range (I-H) = \range H^\perp = \ker H$ (projection onto orthogonal complement of $X$)
	\item $H(I- H) = 0$ (orthogonality)
	\item $X \perp I - H$, ie $X^T (I - H) =(I-H)X = 0$
\end{enumerate}



\section{OLS Residuals and Canonical Change of Basis}
Uses:
\begin{enumerate}
    \item Estimate $\sigma$
    \item Assess adequacy of model (homoskedasticity, distribution)
\end{enumerate}
$$\hat \sigma^2 = \frac{\|y-\hat y\|^2}{n-2},$$
\begin{enumerate}
	\item Columns of $X$ are orthogonal to residuals
	\item  predictions vector is orthogonal to the residuals. 
	\item If intercept, then residuals sum to $0$
	\item 
\end{enumerate}
\subsection{Canonical Change of Basis}

\section{Distributions and testing}
\begin{fact}[Independence of $\hat \beta$ and $\hat \sigma^2$]
    Independence of $\hat \beta$ and $\hat \sigma^2$:
    \begin{proof}
    	
	$$\begin{pmatrix}
		\hat \beta \\
		Y - \hat Y 
	\end{pmatrix} = \begin{pmatrix}
		(X^TX)^{-1} X^T \\
		I_n - H
	\end{pmatrix}Y \sim \calN ((\beta,0)^T, \begin{pmatrix}
		\sigma^2 (X^TX)^{-1} & 0 \\
		0 & \sigma^2 (I - H)
	\end{pmatrix}$$
	    \end{proof}
	    Only relies on normality of errors. We get the same result though as long as $\Cov \epsilon = \sigma^2 I$. 
\end{fact}
From the above we also get that $\hat y = X\hat \beta \indep \hat \epsilon = y - \hat y $. "The residuals are independent of the predictions".


\begin{fact}[Distribution of $\hat \sigma^2$]
$$\hat \sigma^2 = \frac{1}{n} \|y - \hat y\|^2 \sim \frac{\sigma^2}{n} \chi^2_{n-p}.$$
Related to canonical change of basis. 
\end{fact}
\begin{fact}[F-Test]
F-test. Let full model have $p = p_1 + p_2$ predictors and reduced model have $p_1$ predictors (Eg, $p_1 = 1$, $p_2 = p-1$ in the case when reduced is just intercept).
$$\frac{\|\hat y - \hat y_{reduced} \|^2/p_2}{RSS/(n-p_1 - p_2)}\sim F_{p_2, n-p_1 - p_2}$$

Note that $F_{1,n-p} = t^2_{n-p}$ and equivalent to below. See Quals notes section 2.6 for derivation. 
\end{fact}
\begin{fact}[T-test]
$$\frac{\hat \beta_i - \beta_i}{\widehat{SE} (\hat \beta_i)} \sim t_{n-p}, \quad \text{where use estimate }  \hat \sigma^2 = \frac{1}{n-p} \|y-\hat y\|^2,$$
so in testing $\beta_j = 0$:
$$\frac{\hat \beta_j }{\sqrt{ [(X^TX)^{-1} ]_{jj}}RSS/(n-p)}.$$

More generally, for testing $\calH_0: v^T \beta = 0$:

$$\frac{v^T \hat \beta_j }{\sqrt{ [v^T (X^TX)^{-1} ]v}RSS/(n-p)}\sim t_{n-p}.$$
\end{fact}


\begin{definition}[$R^2$]
    $$R^2 = 1 - \frac{\|y - \hat y\|^2}{\| y - \bar y\|^2}.$$
\end{definition}

\section{Generalized Least Squares}
Whitening idea to get into nice least squares world. Sec 7.2 in coaching notes. \myworries{Look back at 2013 Q 4b}
\section{Singular $X$}
If $\rank(X)<p$, then $\hat y$ is unique but $\hat \beta$ \textbf{is not}- take a vector in the null space of $X$ and add to $\hat \beta$. \\
Ways to cope:
\begin{enumerate}
    \item Restrict to \textbf{estimable} functions of $\beta$
    \item Introduce \textbf{side-conditions} on $\beta$
    \item Reparametrize to full rank model

\end{enumerate}
Note that any acceptable way to cope will give:
\begin{enumerate}
	\item The same estimates of estimable functions
	\item The same predictions $\hat y$. As a result, $s^2 = RSS/(n-p)$ is the same. 
\end{enumerate}
Think about it-- not changing the column space of $X$. We project onto column space of $X$. Gives us a $\hat \beta$ that satisfies the normal equations. Rencher 12.3a (pg 309) tells us $\lambda^T \hat \beta$ is invariant under choice of $\hat \beta$ satisfying normal equations. 
\subsection{Estimability}
Eg if neither $\hat \beta_1$ nor $\hat \beta_2$ are uniquely determined, but $\hat \beta_2 - \hat \beta_1$ \textbf{is}. Q: For $c\in \R^p$, for which $c$ is $c\hat \beta$ uniquely determined?  Want:
$$1) X\beta' = X\beta '' \implies c\beta' = c\beta'' $$
equivalently, find $c$ s.t.
$$X\beta = 0 \implies c\beta = 0$$

\begin{definition}[Estimability]
    $c\beta$ \textbf{estimable} if any of the following:
    \begin{enumerate}
        \item $c\hat \beta $ uniquely determined by $\hat y = X\hat \beta $ (even if $\hat \beta$ not unique 
        \item $X\beta = 0 \implies c\beta = 0$
        \item $c \in \operatorname{row}(X)$
        \item $\exists$ $a\in \R^n$ such that $a^T X = c$
        \item $\exists$ \textbf{linear unbiased estimate} of $c\beta $, ie $\exists a\in \R^n$ such that $\E_\beta (a^T y) = c\beta $
    \end{enumerate}
\end{definition}
\begin{theorem}[Gauss Markov]
    Every estimable $c\beta$ has a \textbf{unique}, unbiased, linear estimate which has minimum variance within this class. The estimate is $c\hat \beta$, where $\hat \beta$ is any OLS estimate. \\
    (Assumes homoskedastic noise) 
    \begin{proof}
        (Sketch). Fix $c$ estimable. Exists some $a \in \R^n$ such that $\E a^T y  = c\beta$. Then write $a = a^* + (a- a^*)$ where $a^* = \operatorname{Proj}_{\range X}(a)$. Then $(a^*)^T X = a^TX = c$, show minimum variance:
        
        $$\Var (a^T y) = a^T \Cov y a = \sigma^2 \| a\|^2 = \Var [(a^*)^T y]  + \sigma^2 \|a- a^*\|^2 $$
        
        then show $(a^*)^T y = c\beta $ 
    \end{proof}
    \end{theorem}

    \begin{fact}[Gauss Markov Assumptions]
        Gauss Markov requires no distributional assumptions, just first and second moments of errors.
    \end{fact}
    \subsection{Side Conditions}
    We don't want to just remove one of the features even if its linearly dependent, because then would change interpretation; eg, treatment effects. Instead of imposing $\beta_1=0$ (removing a feature), impose something like $\beta_1 + \beta_2 + \beta_3 = 0$. 
    \begin{fact}[Estimability and Side Conditions]
        Side conditions must be in terms of \textbf{\textit{non-}}estimable functions; ie, constrain the thing we can't estimate uniquely
    \end{fact}
    Let $H\in \R^{s\times p}$ set of side conditions, ie require $H\beta = 0$. 

    \begin{theorem}[Side Conditions, ie when $\exists$ unique $\hat \beta$ satisfying conditions ]
        $X\hat \beta = \hat y$, $H\beta = 0 $ has exactly one solution for any $\hat y\in \range X$ iff: 
        \begin{enumerate}
            \item $\row H \bigcap \row X = \emptyset $ (side conditions not in row space, ie $\vec h_i \beta$ is not estimable 
            \item $\row H \oplus \row X = \R^p$, ie $\rank \begin{pmatrix}
                X\\
                H
            \end{pmatrix} = p$
        \end{enumerate}
        Ie: enough conditions to span the space (part 2) , but not too many (part 1), so that we can still solve $\hat y = X\hat \beta $
    \end{theorem}

    \begin{corollary}[Side conditions make components of $\beta$ estimable]
        If we satisfy the above, then uniqueness from the side conditions $\beta^h$ has estimable components, where $\beta = \beta^h + \beta^x$ where $\beta^h \in (\row H)^\perp   $, $\beta^x \in (\row X)^
\perp $.\\
\myworries{??}
    \end{corollary}
    One idea for incorporating constraints is let $C_0$ be constraints such that $C_0 \beta = 0$, with $C_0 \in \R^{s\times p} $ then let 
    $$C = \begin{pmatrix}
        C_0\\
        C_1
    \end{pmatrix},$$
    so that $C\in \R^{p\times p}$ full rank.
    Write
    $$X\beta = XC^{-1} C \beta = X_1^* \beta_1^* \quad \text { where } \beta_1^* = C_1 \beta\in \R^s $$
    We can \textbf{transform} a rank-deficient $X\in \R^{n\times p}$ to a full rank $X_1^* \in \R^{n\times s}$, so that all the components of $\beta_1^*$ are estimable.



\subsection{ANOVA}
Strats: if you don't know what $\hat \alpha$ should be with certain constraint, easiest to differentiate to find optimal $\hat \alpha$. \\\\

Also, can solve with a different constraint or drop the redundant column if estimating an estimable quantity if that simplifies calculations. Idea is that $\hat \beta$ is different but $c^T \hat \beta$ is the same regardless of choice if $c^T \beta$ estimable. 

\section{Least Squares Computations}
First assume $X$ full rank. Find a $Q$ orthogonal ($\{x_1,\ldots, x_p\} \mapsto \{q_1,\ldots, q_p\}$ ONB, then form $Q$ based on completing the ONB of $\R^n$).  such that:

$$Q^T X = R = \begin{pmatrix}
    \tilde R_{p\times p}\\
    0_{n-p \times p}
\end{pmatrix}$$

Then with $Q^T  y= y^*$,
$$\|y - X\beta \|^2 = \| Q^T y - Q^T X\beta \|^2  = \| y_1^* - \tilde R \beta \|^2 + \|y_2^*\|^2 .$$

Since $X$ full rank, then $\tilde R$ is surjective, so can make the first term $0$. \\
Ie, $X = QR$!\\ 
If $X$ \textbf{not} full-rank, write $X = QRS^T$. $Q$ takes the $p$ columns and finds ONB. Then $S^T$ takes the resultant rows and finds a $r$ dimensional ONB. Yields:

$$\|y-X\beta \|^2 = \| Q^T y - RS^T \beta\|^2 $$
Let $Q^T y = \begin{pmatrix}
    y_1^*\in \R^{r}&\\
    y_2^* \in \R^{n-r}
\end{pmatrix}$
$$\iff \|y-X\beta\|^2 = \|y_1^* - \tilde R \beta_1^*\|^2 + \|y_2^*\|^2 $$
Again make first term $0$ since $\rank \tilde R = r$.
\begin{fact}[Why QR?]
    QR decomposition is useful for the above, since if we can compute it efficiently, it's easy to solve an upper triangular system and we don't have to instantiate $X^T X$. 
\end{fact}
\subsection{Householder Transforms}
\begin{definition}[Householder Transforms (HHT)]
    Any matrix $Q = I - uu^T$ with $\|u\|_2 = 1$ is a HHT.
\end{definition}
\begin{fact}[Properties of HHT]
Some properties:
    \begin{enumerate}
        \item Symmetric
        \item Orthogonal
        \item $u$ is eigen vector with evalue $-1$
        \item All elements of $\{u\}^\perp$ are e-vectors with evalue $1$ (ie, invariant subspace)
    \end{enumerate}
\end{fact}
\begin{fact}[Existence of HHT s.t $a\mapsto b$]
    For any pair of vectors $a,b$ of same length, $\exists$ HHT that transforms $a\to b$. Namely,
    $$u = \frac{b-a}{\|b-a\|}$$
\end{fact}
Our goal here is to transform $X$ via orthogonal matrix to get upper triangular $R$. \\ 
\begin{fact}[QR Decomposition via HHT]
    Recipe:
    \begin{enumerate}
    \item If necessary, permute the columns of $X$ st first $r = \rank X$ are linearly independent (Permutation matrices are orthogonal)
        \item Let $Q_1$ be HHT that takes $x_1 \mapsto \|x_1\|e_1$
        \item Then $Q_1 X$ has first column all 0's except first entry. 
        \item Repeat for the submatrix that is not yet upper diagonal. 
        \item $QX = Q_p \ldots Q_1 X = R$
    \end{enumerate}
\end{fact}
\subsection{Given's Rotation}
\subsection{Gram-Schmidt}
\begin{fact}[Orthogonal Predictors in OLS]
    If predictors are orthogonal:
    $$\hat \beta_j = \frac{\langle y, x_j \rangle }{\|x_j\|^2},$$
    since $X^T X$ is diagonal.\\
    Note this is analogous to if we just have a single predictor $x$ and do regression through the origin. Orthogonal predictors lets us just do regression separately for each predictor. \\
    Further, if $Q$ are orthonormal predictors, then $\hat \beta = Q^T y$.
\end{fact}
The idea here is to convert predictors into orthogonal predictors, solve easy OLS, then convert back. \\ 
GS: $q_i = x_i - \sum_{k=1}^{i-1} \frac{\langle x_i, q_k\rangle }{\|q_k\|^2} q_k $, then $e_i = q_i /\|q_i\|.$ In matrix form:

$$X= \tilde Q_{n\times p} \tilde R_{p \times p},$$ could complete the basis to get a full QR decomp. 
\begin{fact}[Gram Schmidt/OLS connection]
    If we first calculate coefficients $\hat \beta^*$ of $y$ on $\{q_j\}_{j\leq p}$, then $$\hat \beta_{OLS} = \tilde R \hat \beta^* = \tilde R Q^T y.$$
    The idea is we can solve the easy problem in the orthogonalized coordinates, then \textbf{convert back using the upper diagonal matrix from the Gram-Schmidt process}.
\end{fact}
Nice trick is since $\tilde R$ has $1$ on diagonal, then for the \textit{last} OLS coefficient,
$$\hat \beta_p = \hat \beta^*_p  = \frac{y^T q_p} {\|q_p\|^2} \quad \text{ where } q_p = x_p - \sum_{i<p} \frac{\langle x_p, q_k\rangle}{\|q_k\|^2} = x_p - \Proj_{\operatorname{span} x_{(-p)}}(x_p).$$
In general, the coefficient for each $x_j$ is the coefficient in a simple regression of $y$ on $x_j$, but then adjusted for $x_{-j}$: we can always just reorder and get the same solution.
\subsection{Modified Gram Schmidt}
Better numerical stability than regular GS.
\subsection{SVD}
$$X = U_{n\times n} D_{n\times p} V^T_{p\times p} = U_{n\times r} D_{r\times r} V^T_{r\times p}$$
\begin{example}[Given $\{x_i\}_{i\leq n}$, find best fitting hyperplane ]
    $$\min_{\alpha_0,\gamma_i, V:V^T V  =I } \sum_{i=1}^n \|x_i - (\alpha_0 + V\gamma_i)\|^2 $$
We can just solve by the 305c style PCA low rank approx 
    $$\|X_{n\times p} - \Gamma_{n\times p} V_{p\times p}^T \|_F^2 $$
    where $\Gamma = \begin{pmatrix}
        \gamma_1,\ldots ,\gamma_n
    \end{pmatrix}^T$
\end{example}
\begin{example}[Errors in Variables Regression with SVD]
Model:
$$y_i = z_i^T \beta + \epsilon_i, \quad \epsilon_i \sim N(0,\sigma^2_\epsilon) $$
but instead of $z_i$, we actually observe $$x_{ij} = z_{ij} + e_{ij}, \quad e_{ij}\sim N(0,\sigma^2_E) $$
$$\L = \sum_{i=1}^n \frac{y_i - z_i^T \beta}{\sigma^2_\epsilon} + \sum_{j=1}^p \sum_{i=1}^n \frac{(x_{ij} - z_{ij})^2}{\sigma^2_E.}$$

If $\sigma^2_\epsilon = \sigma^2_E$, then def $X^* = [y:X]$, $B = [\beta:I]$, then minimize $\|X^* -ZB\|_F^2 $, ie just a low rank approximation. 
\end{example}
\subsection{Updating/Downdating LS Computations}
\begin{fact}[Woodbury Inversion]
    $$(A+uv^T)^{-1} = A^{-1} - A^{-1} u(I + v^T A^{-1} u)^{-1} v^T A^{-1} $$
\end{fact}
Suppose we've computed LS fit and want to \textbf{update} the fit using a new point. New $(X^TX)^{-1}$:
$$(X^TX + x_{n+1}x_{n+1}^T)^{-1} $$
Update $\hat \beta$ using Woodbury.\\\\
Can use the same trick for downdating (ie LOO fit); $(X^TX - x_ix_i^T)^{-1}$. Idea that $X^T y = X_{(-i)}^T y_{(-i)} + x_i y_i $.

\section{Model Selection (ESL Ch7)}

\section{Cross Validation}
Two possible goals: model selection, model accuracy. Type J question: which model to pick? 

\subsection{CV for Model Selection}
Eg tuning parameter $\lambda$. K-folds. Each data point gets assigned into one of $K$ folds without replacement. Have function $\hat f_{(-k)}$ fit on everything except for data in the kth fold. Use loss function to evaluate the the held out data. Have notion:

$$\hat Err ^{(CV)} (\theta) = \sum_{i=1}^n \text{ all the left out cv errors for each fold}.$$
Use this to pick $\theta^*$. Output a $\hat f(\cdot , \theta^*)$. 

\subsection{Model Accuracy}
I want to use model (eg Lasso with $\lambda = 0.1$)-- suppose we haven't done model selection and are just being told we're using this. I want to know how accurate this model is. Perfect world you'd have extra held out data to test on. 

$$\hat ERR^{(CV)} \quad \text{as above}.$$
is a measure of accuracy. This estimates 
$$Err_n = \E [Err_\tau] = \E [\E [\ell(y_{n+1}, \hat f_\tau (X_{n+1})) | \tau]],$$
where $\tau$ is the training sample. This is the expected error over all possible training dat, ie expectation over random $y,x, \tau$.
But:
$$\E[\hat Err^{(CV)}] > Err_n$$ 
since we're training with not as much data. 

\myworries{Caution}. Always note that CV and bootstrap sample without and with replacement! And we need them to be distinct in CV. 
\begin{definition}[Nested CV]
Idea is to find the best $\theta$ on each fold, so that we have $\theta_1^*,\ldots, \theta_k^*$ and get an out of sample loss with each \textit{best} one. Then average these, and this is an estimate of how we'll do if we choose the best $\theta$ then generalize to new data. 
\end{definition}

Fold sizes: LOO is less biased, but can introduce more variance. This is because our models are really similar, which leads to correlation in the error estimates, which leads to higher variance. Also, computational issues.

\subsection{CV with dependent samples}
If you have correlations you end up with smaller error estimates because there's bleed between your folds-- ie if you have the same person with different observations of them in different folds. Your model has already seen that person and so will generalize better to more data on that person. \\\\
Or temporal data-- if know whether on June 2 in training, then that improves our out of sample prediction to June 3. \\\\

Solution is to block things together ala block bootstrap. Ie make the folds dependent, dependent outside of the fold. \\\\

For a single person, put all of their observations in a single fold. 
\section{Regularization (Ridge, Lasso)}
\subsection{Ridge}
Ridge objective:
$$\hat \beta_{ridge} = \arg \min _\beta \frac{1}{2} \|y - X\beta\|^2 + \lambda \|\beta\|_2^2 = (X^TX + \lambda I_p )^{-1}X^T y $$
Degrees of freedom:
$$df = \sum_{j=1}^p \frac{\sigma^2_j}{\sigma^2_j + \lambda} \quad \text{ where } \sigma_j \text{ are singular values of } X.$$  
\subsection{Lasso}

Lasso objective: 
$$\hat \beta_{lasso} = \arg \min _\beta \frac{1}{2} \|y - X\beta\|^2 + \lambda \|\beta\|_1$$ 

\newpage

\section{GLMs}

$\eta = g(\mu)$ where $\mu = EY$. 


Typical thing is $y_i \sim f_{\mu_i}$. 


\section{Bayesian GLMs}
For estimating posterior mode: gradient ascent on the joint. 

\subsection{Estimating Posterior Distribution/Expectations under intractable Posterior}
\begin{enumerate}
	\item MCMC
	\item VI 
\end{enumerate}
\section{Bootstrap}
\subsection{Confidence intervals}
How to construct a confidence interval, estimate standard deviation, etc but don't know much about distribution. Key idea is that we're going to resample from our dataset \textbf{with replacement}. N datapoints in the sample, then we sample N samples with replacement. 

$$X_1,\ldots, X_n \simiid P_\theta, \quad \text {want to estimate } \theta.$$

If I have an estimate $\hat \theta$, can I give a confidence interval for $\theta$? 

\begin{enumerate}
	\item For $b=1,\ldots, B$, sample $x_1^{(b)},\ldots, x_{n}^{(b)}$ uniformly with replacement.
	\item Compute $\hat \theta^{(b)}$ 
\end{enumerate}

Now we have a collection $\{\hat \theta^{(b)} - \hat \theta\}$, can get quantiles of these empirical differences. Gives $\hat q_{\alpha/2} \hat q_{1-\alpha/2}$. 

$$\hat C = [\hat \theta - q_{1-\alpha/2}, \hat \theta -\hat q_{\alpha/ 2}].$$

Can show that empirical distribution $\hat \theta ^{(b)}- \hat \theta$ is "close" to distribution of $\hat \theta - \theta$ under some conditions on $\hat \theta$. \\\\

\footnote{The "percentile" boostrap just looks at $\theta \in [\tilde q (\alpha/2), \tilde q(1-\alpha/2)]$ where $\tilde q$ is quantiles of just the bootstrapped $\hat \theta$. This interval is 

$$C\in [\hat \theta + \hat q_{\alpha/2}, \hat \theta + \hat q _{1-\alpha/2}],$$
so equal in the case of symmetric $\hat q$. }


\subsection{Testing}
In a composite null, parametric setup. 
$$\calH_0: X_i \simiid P_{\theta} \text{ for some } \theta\in \Theta_0$$

\begin{enumerate}
	\item Choose test statistic $T(X_1,\ldots, X_n)$. 
	\item Use consistent estimator $\hat \theta \conprob \theta$ under $P_\theta$. 
	\item Sample $X_1^{(b)}, \ldots, X_n^{(b)} \simiid P_{\hat \theta}$
	\item Compute $T^{(b)}$
	\item Check if $T$ is in the top $\alpha$ quantile of $\{T,T^{(1)},\ldots, T^{(B)}\}$. 
\end{enumerate}



\subsection{Block bootstrap}
Split data into $k$ blocks. Create the blocks such that any dependency happens within block. For each bootstrap sample $b$, sample $k$ blocks with replacement. Take the entire block at a time. 


\subsection{Bootstrap for Regression}
\myworries{Check this} ???\\ \\ 

Also \textbf{bootstrapped residuals} which leads to no invertibility issues. 
\section{EM}
Want to know how to identify these problems and approach for solving. \\\\

When to use. Scott will literally tell you to derive the E and M step. For the older ones, it doesn't tell you to. You should look for:\textbf{ Mixture models }(ie latent variable in disguise). Wishful thinking-- "if I knew x, I could calculate y", and vice versa. 

\begin{example}[$X_i \sim \pi_0 N(0,1) + (1-\pi_0) N(\mu,1)$]
We'd like to estimate $\pi_0$ and $\mu$. Our instinct is to do maximum likelihood but there's non-concavity (\myworries{??}). The natural latent variable to introduce is $z_i = \Ind[\text{ index i is a non-null}]$. Then $z_i\simiid Bern(1-\pi_0)$ and $x_i |z_i=0 \sim N(0,1)$, and $x_i | z_i=1 \sim N(\mu,1)$. 
\end{example}

\begin{recipe}[EM Recipe]
To do
\begin{enumerate}
	\item Find a latent variable
	\item Write complete log likelihood $p(x,z;\theta)$-- include latent and observed.
	\item Probably compute posterior 
	\item E-step: Compute expected log likelihood wrt posterior $\tilde \ell (\theta, \theta^{(t)}) = \E_{q^{(t)}} \ell(\theta;x,z)$. Ie under $q= p_{\hat \theta^{(t)}} (z|x)$.
	\item M-step: arg max the expected log joint $\ell (\theta, \hat \theta^{(t)})$.  
\end{enumerate}
\end{recipe}
\end{document}