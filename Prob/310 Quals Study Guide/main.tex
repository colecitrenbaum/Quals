\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{hw}
\usepackage{tcolorbox}
\usepackage[ruled,vlined]{algorithm2e}

% ------------------  Bibliography  ------------------
%\usepackage[
%  backend=biber,
%  style=numeric,      % plain numbers: [1]
%  sorting=none,       % keep refs in citation order
%  maxbibnames=99      % show all authors in bib
%]{biblatex}
\usepackage{booktabs}
% ----------------------------------------------------

\tcbuselibrary{breakable}
\title{310 Quals Strategy Compendium}
\newcommand{\answer}[1]{%
  \begin{tcolorbox}[
    colback=gray!9.8,
    boxrule=0.5pt,
    breakable]
  \small #1
  \end{tcolorbox}}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\newcommand{\simiid}{\overset{iid}\sim }
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\row}{\operatorname{row}}
\newcommand{\Proj}{\operatorname{Proj}}

\begin{document}
\maketitle

\section{Permutation and counting facts}
\begin{fact}[Number derangements of $k$-element set]
Derangements: $D_n$ is the number of permutations with no fixed points.\\

Via inclusion exclusion.
$$D_k = k! \sum_{j=0}^{k} \frac{(-1)^j}{j!}.$$
Eg, if $T$ is number of fixed points:
$$P(T=k) = \frac{1}{n!} \binom{n}{k} D_{n-k},$$
since the remaining $n-k$ must \textbf{not} be fixed.

    
\end{fact}
Apply the above to "Distribution of number of fixed points"- type questions. 

\begin{fact}[Catalan Numbers]
	Catalan Numbers: 
	
	$$C_n = \frac{1}{n+1} \binom{2n}{n} = \frac{(2n)!}{(n+1)! n!} \quad n\geq 0$$
	
	
		Dyck Paths 
\end{fact}

\begin{definition}[Cycles]
	Cycle of a permutations
\end{definition}

\begin{definition}[Descents]
	
\end{definition}

Reference: check Persi and Susan's paper


\section{Distribution Facts}
\subsection{Gaussian Facts}
\begin{fact}[Max of Gaussians and fluctuations]
	Max of Gaussians $\sqrt{2\log n}$ with fluctuations $1/\sqrt{\log n} $
\end{fact}
\begin{fact}[Max of sub-gaussian] 
	Expectation upper bound applies to correlated sub-gaussian reandom variables -- see Vershynin. 
\end{fact}
\begin{fact}[Mill's Ratio]
	
\end{fact}

\begin{fact}[Normal Conditional Distributions]
	$$(X,Y) \sim MVN(\mu, \Sigma \implies X|Y \sim \ldots $$
\end{fact}

\subsection{Poisson, Exponential Distribution}
Superposition and thinning.

\begin{fact}[Superposition]
	See Poisson with integer mean? Try superposition:
	
	$$Pois(n) = \sum_{i=1}^n Pois(1)$$
\end{fact}

\begin{fact}[Renyi Representation of Exponential]
	
\end{fact}
\begin{fact}[Maximum, minimum of Exponential]
	
\end{fact}


\section{Basic set theory and measure theory}
\begin{definition}[Sigma Algebra]
\end{definition}
\begin{definition}[Algebra/Field]
\end{definition}





\begin{definition}[Outer measure]
Defined by 
\begin{enumerate}
	\item A non-negative set function 
	\item ......
\end{enumerate}
The typical outer measure is 
\myworries{WRite this out}
\end{definition}

\begin{definition}[Measurable sets]
$E$ is $\mu^*$ measurable if for all $B\subset \Omega$:

$$\mu^* (B) = \mu^*(B\cap A) + \mu^*(B \cap A^c).$$
\end{definition}

Write about littlewood's principles here 

\begin{example}[Non-measurable sets]
\myworries{Todo}
\end{example}

\section{Pi-Lambda and Good Sets}

\begin{definition}[$\pi$-system]
Collection of sets that is closed under \textbf{finite intersections}
\end{definition}
\begin{definition}[$\lambda$-system]
$L$ lambda system if
\begin{enumerate}
	\item $\Omega \in L$
	\item closed under complements
	\item closed under countable disjoint unions
\end{enumerate}
Alternative definition:
\begin{enumerate}
	\item $\Omega \in L$
	\item $A,B \in L$ and $A\subset B$ then $B\setminus A \in L$
	\item $A_1, A_2,\ldots, \in L$ an increasing sequence of sets, then $\bigcup_i A_i \in L$
\end{enumerate}
\end{definition}
Note that a collection is a $\sigma$-algebra $\iff$ it is both a pi system and a lambda system.  
\begin{theorem}[$\pi-\lambda$ Theorem]
If $P$ is a $\pi$ system and $L$ is a $\lambda$-system with $P\subset L$, then $\sigma(P)\subset L$.\\
Use to proof uniqueness of extension from an algebra to the sigma field.
\end{theorem}
\begin{example}[Quals 2017, Question 2]
\end{example}


\begin{theorem}[Monotone Class Theorem]
Use def:
\begin{definition}[Monotone Class]
$M$ is a monotone class if 
\begin{enumerate}
	\item Closed under increasing unions
	\item Closed under decreasing intersections
\end{enumerate}
\end{definition}
If an algebra $A$ is contained in a monotone class $M$, then $\sigma(A) \subset M$.
\end{theorem}
\begin{theorem}[Monotone Class for functions]
\myworries{Double check this}\\
$M$ be a vector space of $\R$-valued functions on $\Omega$ such that
\begin{enumerate}
	\item $1 \in M$
	\item $M$ is a vector space over $\R$. Ie closed under addition and scalar mult
	\item If $h_n\geq 0\in M$ and $h_n\uparrow h$, then $h\in M$.
\end{enumerate}
Then if $P$ is a pi system such that $\Ind_A \in M$ for all $M\in P$, then $M$ contains all functions that are measurable with respect to $\sigma(P)$. 
\end{theorem}
Ie want to prove something about some functions measurable on $\sigma(P)$... 



\begin{theorem}[Independent $\pi$ systems generate independent sigma algebras]
$$\{C_i\}_{i\in I} \text { independent} \implies \{\sigma(C_i)\} \text{ independent} $$
\begin{example}[Prove two random variables independent]
Check that generating pi systems are independent, ie $P(X\leq b, Y\leq a) = \prod P(X\leq b) P(Y\leq a)$. 
\end{example}

\begin{definition}[Independence of (uncountable) collections of sets]
$\{A_\alpha\}_{\alpha \in I}$ each be a collection of sets. Independent if any \textbf{finite subcollection }are mutually independent. 
\end{definition}

\end{theorem}
\section{Extension theorems}
\begin{theorem}[Caratheodory]
Extend a measure on an algebra to a $\sigma$ algebra, uniquely if finite.\\ \\

The important idea is make an outer measure, then prove the lemma is that the $\mu^*$ measurable sets in $\calF$ for a sigma algebra on which $\mu^*$ is countably additive, ie a bonafide measure. 
\end{theorem}
\section{Random variables}
\begin{definition}[Random variable]
$X: \Omega \to \bbS$ is measurable if:

$$X^{-1}(B):= \{\omega: X(\omega) \in B\} \in \calF \quad \forall B\in \calS$$
Fix an arbitrary set in the co-sigma algebra and check that its preimage is measurable-$\calF$. 
\end{definition}
\begin{recipe}[Measurability]
If $\cal S = \sigma(\cal A)$ and $X^{-1} (A) \in \calF$ for all $A\in \calA$, then $X$ is measurable. \\\\
Note-- nothing necessary about it being a pi system, just pick any convenient generators.
\begin{proof}
	Such sets form a sigma algebra and $\cal S = \sigma(\cal A)$. 
\end{proof}
\end{recipe}


\begin{definition}[Characterize distribution functions of real-valued random variable]
Dembo Theorem 1.2.37. 
\begin{enumerate}
	\item Non decreasing
	\item $\lim_{x\to \infty} F(x) = 1$ and $\to -\infty$ gives $0$
	\item $F$ right continuous  
\end{enumerate}
\end{definition}

\section{0-1 Laws}
\begin{definition}[Tail field]
Defining $\calT_n = \sigma(X_r , r>n)$ and the tail sigma algebra of the process is $\calT = \cap_n \calT_n$.  
\end{definition}
\begin{theorem}[Kolmogorov 0-1]
The tail sigma field is P-trivial. Dembo 1.4.10
\end{theorem}

\section{Borel Cantelli} 
\begin{example}[Longest head runs]
See Durrett. 
\end{example}
A helpful idea is splitting into blocks.\\ \\

\textbf{Truncation} arguments. Want to show something about $\{X_n\}$. Consider:

$$Y_n = X_n \Ind[|X_n| \leq c_n],$$
which will then be integrable. Then prove something about $Y_n$, you can transfer this knowledge to knowledge about $X_n$ by the following idea:

$$P(X_n \neq Y_n \text{ io}) =0 \text { if } \sum_{i=1}^n P(|X_n| >c_n) <\infty$$ 

So if the above is zero, eventually $X_n = Y_n$, so the limiting behavior of $Y_n$ is the same as that of $X_n$. The difficulty is picking a $c_n$ such that the above holds. 

\begin{theorem}[Borel Cantelli 3]
\myworries{Not as useful but still check these}\\
If you have a filtration $\{F_n\}$ and $A_n \in \calF_n$, then:
\begin{enumerate}
	\item ( analog of Borel Cantelli 1)
	$$\sum_{k=1}^\infty P(A_k | \calF_{k-1} )(\omega) <\infty \implies \sum \Ind[\omega \in A_k\} <\infty$$ 
	\item Analog of BC 2
	$$\sum_k P(A_k|\calF_{k-1} )(\omega) = \infty \implies \frac{\sum \Ind[A_k (\omega)]}{\sum P(A_k |\calF_{k-1})(\omega)|} \to 1$$
\end{enumerate}

\end{theorem}

\section{Modes of Convergence}
\begin{recipe}[Proving as convergence]

Some ideas:
\begin{enumerate}
	\item BC 1: check that $P(|X_n - X| > \epsilon \text{ i.o} ) = 0$. Consider \textit{subsequence trick} below. 
	\item If $X_n \conprob X$, then $X_{n_k} \conas X$ for a subsequence. (Good for counter examples) 
	\item $X_n\conprob X$ and $X_n$ is monotone (ie for all $\omega$, $X_n(\omega)$ is increasing in $n$), then $X_n \conas X$.
	\item Continuous mapping theorem 
	\item Skorohod's Representation. If $X_n \implies X$ weakly, then there exists a probability space and random variables $Z_n =^d X_n$ and $Z=^d X$ such that $Z_n \conas Z$. (Good for counter examples) 
\end{enumerate}
\end{recipe}

\begin{theorem}[BC Subsequence trick]
Sometimes $\sum P(A_n) =\infty$. 
\begin{enumerate}
	\item Pick a subsequence such that it's finite. 
	\item Shows that $A_{n_k}$ infinitely often occurs with probability $0$.
	\item Apply interpolation or some other argument for everything in between, ie to conclude that $A_n$ infinitely often wp $0$ also. 
	
\end{enumerate}
Eg for monotone $X_n$:

$$ \frac{X_n}{\E X_n} \leq \frac{X_{n_{k+1} -1}}{\E X_{n_k}}$$
So $\limsup_k \frac{X_{n_{k+1} -1}}{\E X_{n_k}} \leq 1 \implies \limsup_n \frac{X_n}{\E X_n}\leq 1$. We can control the left hand side by controlling the right hand side along our choice of subsequence. 

\myworries{See Dembo notes subsequence section}
Examples-- SLLN, Brownian LLN, LIL. \\ \\

\myworries{SLLN in Persi notes and Renewal theorem in Dembo}

\end{theorem}

\begin{recipe}[Proving conv in prob]

Some ideas:
\begin{enumerate}
	\item Show almost sure convergence
	\item Show convergence in $L^p$ for $p\geq 1$ (in $L^2$ in particular useful):
	\begin{itemize}
		\item Show that $\E X_n \to \mu$, $\Var X_n \to 0$, then $X_n \to \mu$ in $L^2$
	\end{itemize}
	\item CMT 
	\item $X_n \condist c$ constant
	\item Truncation tool: \textbf{Weak law for Triangular Array}. For when we don't have second moments. 
	\end{enumerate}
\end{recipe}

\begin{theorem}[Weak law triangular array - 2.1.11 Dembo]
\myworries{Check this}
Suppose triangular array $\{X_{n,k}\}_{{n\in \N, k\leq n}}$ of pairwise independent rv. Define a truncated array with the same \textit{within-row} truncation:
$$\bar X_{n,k} = X_{n,k} \Ind[|X_{n,k}| < b_n ]$$. If we have two conditions:

$$\sum_{k=1}^n P(|X_{n,k}|>b_n) \to^n 0,$$
and,
$$b_n^{-2} \sum_{k=1}^n \Var (\bar X_{n,k}) \to 0,$$
Then:

$$b_n^{-1} (\sum_{k=1}^n X_{n,k} - a_n) \conprob 0 \quad \text{ where } a_n = \sum_{k=1}^n \E \bar X_{n,k}.$$

In case of St Petersburg paradox, then show that $a_n/b_n \to 1/2$ so that we get $b_n^{-1} \sum_{k=1}^n X_k = (b_n^{-1} \sum_{k=1}^n X_{n,k}) \conprob 1/2 $. 



\end{theorem}
\begin{recipe}[Prove $L^p$ convergence]
Ideas - see session notes 
\begin{enumerate}
	\item $X_n \conas X$ and $X_n$ uniformly bounded then $X_n \to^{L1} X$.
	\item If $X_n$ uniformly bounded then $X_n\condist 0 \implies X_n \to^{L1} 0$.
	\item If $|X_n|^p$ is UI then $X_n \condist X \implies \E |X_n|^p \to E|X|^p$ 
	\item Scheffe's Lemma 
\end{enumerate}
\end{recipe}

\begin{recipe}[Proving UI]
Try
\begin{enumerate}
	\item Sums of UI sequence of rvs are UI
	\item Domination: If $E\sup |X_i| < \infty$ then $\{X_i\}$ is UI
	\item If $\{X_i\}$ is UI, then $\sup E |X_i| <\infty$ but not the converse. 
	\item Prop 8.3.3: $X_n$ UI $\iff$ $\sup E|X_i| <\infty$ and for all $\eps$, there exists $\delta$ for all A such that $P(A) <delta$ then $\E[|X_n| \Ind[A] ] <\eps$
	\item If $\sup \E |X_i|^r <\infty$ if $r>1$, then UI 
	\item $X\in L^1$, then $\{\E[X|\calG]: \calG \subset \calF\}$ is UI
	\item $L^1$ convergence implies UI
\end{enumerate}
\end{recipe}

\section{Integration}
\begin{definition}[Lebesgue Integral]
$$\int_\Omega f(\omega) d\mu(\omega) = \sup \left (\sum_{i=1}^n \nu_i \mu(A_i)\right)$$
where $\nu_i = \inf_{\omega \in A_i} f(\omega)$ and the sup is over all partitions of $\Omega$.
\end{definition}
\begin{theorem}[MCT]
Note that can also do for general functions (not necessarily non negative) so long as $f_n \geq g \in L^1$.
\end{theorem}
\begin{theorem}[Fatou's Lemma]
$$\int \lim \inf_n f_n d\mu \leq \lim \inf_n \int f_n d\mu $$
\end{theorem}
\begin{theorem}[DCT]
If $f_n \to f$ a.e $\omega$, and there exists $g$ such that $|f_n(\omega)|\leq g(\omega)$ a.e $\omega$ and $\int gd\mu <\infty$ then exchange integral and limit. Must dominate the sequence. 
\end{theorem}

\begin{theorem}[Scheffe's Lemma]
Is a statement about combining $L^1$ and as convergence. If $f_n \conas f\in L^1$, then:
$$\| f_n -f\|_{L^1} \to 0 \iff \int |f_n| d\mu \to \int |f|d\mu $$
We always have that convergence in $L^1$ implies convergence of the expectations (without any a.s. convergence assumption) , but the other direction is Scheffe's contribution. 
\end{theorem}


%\begin{theorem}[Generalized DCT]
%\myworries{????}
%If $|f_n|\leq g_n$ such that $g_n \to g \in L^1$ (convergence in $L^1$) then $\int f_n d\mu \to \int fd\mu$
%\end{theorem}

\begin{theorem}[Reverse Fatou]
If $f_n \leq g \in L^1$ then :

$$\lim \sup_n \int f_n d\mu \leq \int \lim \sup f_n d\mu$$
\end{theorem}

\begin{fact}[$L^p$ spaces nested]
$$\|Y\|_r \leq \|Y\|_q$$
\end{fact}

\begin{fact}[$L^q$ convergence fact - Dembo 1.3.28]
$X_n\to^{L^q} X \implies \E|X_n|^q \to \E|X_\infty|^q$ for any $q$. (Minkowski)\\\\ Also for only $q\in \N$, $\E X_n^q \to \E X_\infty^q$. (some wild algebraic shit for odd $q$).

\end{fact}
\begin{theorem}[Holder's]
If $p,q>1$ with $1/p + 1/q = 1$ then 
$$E|XY| \leq \|X\|_p \|Y\|_q $$
Cauchy Schwarz is special case.
\end{theorem}

\begin{theorem}[Minkowski]
Triangle inequality for the $\|\cdot \|_p $ norm
\end{theorem}

\begin{definition}[Uniform integrability (UI)]
Possibly uncountable collection $\{X_\alpha: \alpha \in I\}$ is called UI if 

$$\lim_{M\to\infty} \sup_{\alpha \in I} \E[|X_\alpha| \Ind[|X_\alpha | >M] = 0$$
\end{definition}
\begin{fact}[Dominated implies UI]
If $|X_\alpha|\leq Y$ for integrable Y, then collection is UI. \\\\

As a corollary, any finite collection of integrable rv is UI.
\end{fact}

\begin{theorem}[Vitali Convergence Theorem]
Supposing that $X_n \conprob X$, then:
$$\{X_n\} \text{ is UI} \iff X_n \to^{L1} X \iff X_n \text{ is integrable for all } n\leq \infty \text{ and } E|X_n| \to E|X_\infty|.$$
\end{theorem}




\section{Product $\sigma$-algebras}
Existence of unique product measure of $n$ $\sigma-$ finite measures. \\\\

\begin{theorem}[Kolmogorov Extension]
Unique probability measure on $(\R^\N, \calB_c)$ with correct FDDs. 
\end{theorem}



\begin{theorem}[Fubini's]
Conditions: $h\geq0$ or $\int |h| d\mu <\infty $ where $\mu = \mu_1 \times \mu_2$. 
\end{theorem}
%% Weak Convergence
\section{Weak Convergence}
\subsection{Methods}
\begin{enumerate}
	\item \textbf{Direct}. Show that $F_n(x) \to F(x)$ for all continuity points.
	\item If there's a density, try to show that $f_n(x) \to f_\infty$ and check that $f_\infty$ a valid pdf. 
	\item If $X_n \geq 0$, show that $\int_0^\infty \exp(-\lambda x) d\mu_n (x) \to L(\lambda) = \int_0^\infty \exp(-\lambda x) d\mu_\infty (x)$. Note that $L(\lambda)$ is a Laplace transform of some $\mu$ if $L(\lambda)\downarrow 1$ as $\lambda\downarrow 0$. (Just need for positive $\lambda$. 
	\item MGFs
	\item Characteristic functions- show that $\phi_n(t) \to \phi(t)$ for all $t\in \R$. ($\phi$ is a characteristic function of a probability measure if $\psi(t) \to 1$ as $t\downarrow 0$. 
	\item CLT 
\end{enumerate}

\begin{example}[Cycling of Random Number Generators (2007 Q2)]
Similar to birthday problem. 
\answer{

\begin{align}
	P(T>k) & = \prod_{i=1}^k (1-\frac{i}{n})\\
	&\approx \prod \exp(-1/n)\\
	& \approx \exp(-k^2/n).
\end{align}
So $P(T> x\sqrt{n}) \approx \exp(-x^2/2)$ should work.\\\\
Need to justify this rigorously. To do so, use $|\log (1+x) - x| < Cx^2$ when $|x| <1/2$. Ie, $\log (1+x) = x + O(x^2)$. \\\\ 

See lecture 1 in 310a. 

}
\end{example}

%%%%% CLT %%%%%%
\section{CLT}
Heuristic: "not too dependent", "no few terms dominate". 
\begin{theorem}[Lindeberg CLT]
Suppose we have a triangular array such that:

\begin{enumerate}
	\item  for fixed $n$, $\{X_{ni}\}_{i=1}^{k_n}$ are independent (ie independence within row). 
	\item Suppose also $\E (X_{ni}) = 0$ for all, and $\Var X_{ni} = \sigma^2_{ni} <\infty$.
	\item Define $S_n = \sum_{i=1}^{k_n} X_{ni}$ and $s_n^2 = \sum_{i=1}^{k_n} \sigma^2_{ni}$ (sum of rows)
	\item \textbf{Lindeberg condition} holds ie for all $\eps>0$:
	$$\lim_{n\to \infty} \frac{1}{s_n^2} \sum_{i=1}^{k_n} \int X_{ni}^2 \Ind[ |X_{ni} | >\epsilon s_n ] dP = 0$$
\end{enumerate}  

Then:

$$\frac{S_n}{s_n} \condist \calN(0,1).$$

Note that if not mean zero, subtract off the means:

$$\frac{S_n - \sum_{i=1}^{k_n} \mu_{kn}}{s_n} \condist N(0,1)$$

\end{theorem}

\begin{example}[CLT failures: too wild]
$$X_i = \begin{cases}
	0 \text{ wp } 1-1/i\\
		1 \text{ wp } 1/i
		\end{cases}$$
		
The issue is that some $X_i$'s  dominate-- ie the big ones.
\end{example}

\begin{example}[CLT Failures: too dependent]
\end{example}


\begin{recipe}[CLT for non-square integrable]
Session 4 notes. Similar to convergence in probability strategy. \\\\
Assume $X_{n,k} \ in L^1$ and exist $c_n$ such that 

\begin{enumerate}
	\item $\sum_{k=1}^{\ell_n} P(|X_{n,k}| > c_n) = o(1)$
	\item Lindeberg condition satisfied for truncated $Y_{n,k} =X_{n,k} \Ind [X_{n,k} \leq c_n]$
	\item $\sum_{k=1}^{\ell_n} (\E X_{n,k} - \E Y_{n,k}) = o(s_n)$ where $s_n^2$ sum of variances of truncated in the $n$-th row.
\end{enumerate}
Then $$\frac{\sum_{k=1}^{\ell_n} X_{n,k} - \E X_{n,k}}{s_n} \condist N(0,1)$$
\end{recipe}
\begin{theorem}[Lyapunov CLT]
Lyapunov condition is sufficient for Lindeberg's condition. Same setup, check that:


$$s_n^{-2-\delta} \sum_{i=1}^{k_n} \E |X_{n,k} - \E (X_{n,k})|^{2+\delta} \to 0 \quad \text{for some } \delta > 0$$
\end{theorem}


%%% Characteristic function %%

\section{Characteristic Functions}
\begin{definition}[Characteristic Function]
Characteristic function is fourier transform of $\mu$. 

$$\phi(t) = \E [\exp(itX)].$$


\begin{recipe}[Characteristic Function Chaos]
Try to get $\phi(t) \approx (1+ \frac{f(t)}{n})^n$ form so that we can use exp limit. 
\end{recipe}
\end{definition}




\section{Stein's Method (Poisson)}




\subsection{Method 1 - Dependency Graphs}

\subsection{Method 2 - when dependency graph doesn't work (ie complete)}

\begin{example}[Fixed Points - 310a HW8]
    Let $\sigma$ be a uniformly chosen permutation in the symmetric group $S_n$. Let $W = \#\{i : \sigma(i) = i\}$ (the number of fixed points in $\sigma$). Show that $W$ has an approximate Poisson(1) distribution by using Stein's method to get an upper bound on $\|P_W - \text{Poisson}(1)\|$. (Hint: see section 4.5 of Arratia-Goldstein-Gordon.) Give details for this specific case.

\answer{
Let $I = [n]$. We choose $B_\alpha = \{\alpha\}$ and use Theorem 1 from Arratia-Goldstein-Gordon. 

For each $i\in I$, let

$$X_i = \begin{cases}
    1 \text{ if } \sigma(i) = i\\
    0 \text{ otherwise}
\end{cases}.$$
Naturally, $P(X_i =1) = \frac{1}{n}$. We let $W = \sum_{i\in I} X_i$ and $\lambda = E[W] = 1$. We now use Stein's method as given in Arratia-Goldstein-Gordon Theorem 1 to get an upper bound on $\lVert P_W - Pois(1)\rVert$.
$$b_1 = \sum_{\alpha\in I} \sum_{\beta \in B_\alpha} p_\alpha p_\beta = \sum_{\alpha \in I} p_\alpha^2 = \frac{1}{n}.$$
Next, because we let $B_\alpha = \{\alpha\}$, 
$$b_2 = 0.$$
Finally, for the third term, by Lemma 2 (p 418) in Arratia et al,
$$b_3 \leq \min_{1<k<n} (\frac{2k}{n-k} + 2n 2^{-k} e^{e})\sim 2\frac{(2log_2 n + e/ln 2)}{n},$$
due to the fact that $\lambda=1$ in our problem, so $\lambda = o(n)$

Now note that as $n\to \infty$, $b_1\to 0$ and $b_3\to 0$, so, noting that the Arratia paper's definition of TV distance is twice our definition of TV distance:

$$\lVert P_W - Pois(1)\rVert\leq b_1 + b_2 + b_3 = \frac{1}{n} + \frac{4\log_2 n + 2\epsilon/\ln 2}{n} + o(1)$$
Now as $n\to \infty$, $b_1 \to 0$ and $b_3 \to 0$, so $\lVert P_W - Pois(1)\rVert \to 0$. 
}
\end{example}

\begin{example}[Near Fixed Points- 2004 Q2]
    
\end{example}



\newpage 
\section{Approximations}

$$1-x \leq e^{-x} \quad 1-x\geq e^{-2x} \quad \text{both for small } x?$$  
$$\log(1+x) = x + O(x^2)) \quad \text{ for small } x$$


\subsection{Binomial Coeffs and Stirlings}
$$(\frac{n}{k})^k \leq \binom{n}{k} \leq (\frac{ne}{k})^k $$
\myworries{Stirlings}


\section{Misc}
\begin{definition}[Metric]
\end{definition}



\end{document}