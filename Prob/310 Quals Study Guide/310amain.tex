\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{hw}
\usepackage{tcolorbox}
\usepackage[ruled,vlined]{algorithm2e}

% ------------------  Bibliography  ------------------
%\usepackage[
%  backend=biber,
%  style=numeric,      % plain numbers: [1]
%  sorting=none,       % keep refs in citation order
%  maxbibnames=99      % show all authors in bib
%]{biblatex}
\usepackage{booktabs}
% ----------------------------------------------------

\tcbuselibrary{breakable}
\title{310 Quals Strategy Compendium}
\newcommand{\answer}[1]{%
  \begin{tcolorbox}[
    colback=gray!9.8,
    boxrule=0.5pt,
    breakable]
  \small #1
  \end{tcolorbox}}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\newcommand{\simiid}{\overset{iid}\sim }
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\row}{\operatorname{row}}
\newcommand{\Proj}{\operatorname{Proj}}

\begin{document}
\maketitle

\section{Permutation and counting facts}
\begin{fact}[Number derangements of $k$-element set]
Derangements: $D_n$ is the number of permutations with no fixed points.\\

Via inclusion exclusion.
$$D_k = k! \sum_{j=0}^{k} \frac{(-1)^j}{j!}.$$
Eg, if $T$ is number of fixed points:
$$P(T=k) = \frac{1}{n!} \binom{n}{k} D_{n-k},$$
since the remaining $n-k$ must \textbf{not} be fixed.

    
\end{fact}
Apply the above to "Distribution of number of fixed points"- type questions. 

\begin{fact}[Catalan Numbers]
	Catalan Numbers: 
	
	$$C_n = \frac{1}{n+1} \binom{2n}{n} = \frac{(2n)!}{(n+1)! n!} \quad n\geq 0$$
	
	
		Dyck Paths 
\end{fact}

\begin{definition}[Cycles]
	Let $s \in S_n$ be a permutation. Then $L_i(s) = \inf \{j: s^j(i) = i\}$. Ie the first time we come back to $i$. See pg 101 Dembo remark. Durret Ex 2.2.4. Then the cycle is the collection $\{s^j(i): 1\leq j\leq L_i(s)\}$-- ie the elements of $[n]$ before we come back to $i$. \\\\
	If $T_n = \# \text{cycles}$, $(T_n - \log n)/\sqrt{\log n} \condist \calN(0,1)$. 
\end{definition}

\begin{definition}[Descents]
	
\end{definition}

Reference: check Persi and Susan's paper


\section{Distribution Facts}
\subsection{Gaussian Facts}
\begin{fact}[Max of Gaussians and fluctuations]
	Max of Gaussians $\sqrt{2\log n}$ with fluctuations $1/\sqrt{\log n} $
\end{fact}
\begin{fact}[Max of sub-gaussian] 
	Expectation upper bound applies to correlated sub-gaussian reandom variables -- see Vershynin. 
\end{fact}
\begin{fact}[Mill's Ratio]
	
\end{fact}

\begin{fact}[Normal Conditional Distributions]
	$$(X,Y) \sim MVN(\mu, \Sigma \implies X|Y \sim \ldots $$
\end{fact}

\subsection{Poisson, Exponential Distribution}
Superposition and thinning.

\begin{fact}[Superposition]
	See Poisson with integer mean? Try superposition:
	
	$$Pois(n) = \sum_{i=1}^n Pois(1)$$
\end{fact}

\begin{fact}[Renyi Representation of Exponential]
	
\end{fact}
\begin{fact}[Maximum, minimum of Exponential]
	
\end{fact}


\section{Basic set theory and measure theory}
\begin{definition}[Sigma Algebra]
\end{definition}
\begin{definition}[Algebra/Field]
\end{definition}





\begin{definition}[Outer measure]
Defined by 
\begin{enumerate}
	\item A non-negative set function 
	\item ......
\end{enumerate}
The typical outer measure is 
\myworries{WRite this out}
\end{definition}

\begin{definition}[Measurable sets]
$E$ is $\mu^*$ measurable if for all $B\subset \Omega$:

$$\mu^* (B) = \mu^*(B\cap A) + \mu^*(B \cap A^c).$$
\end{definition}

Write about littlewood's principles here 

\begin{example}[Non-measurable sets]
\myworries{Todo}
\end{example}

\section{Pi-Lambda and Good Sets}

\begin{definition}[$\pi$-system]
Collection of sets that is closed under \textbf{finite intersections}
\end{definition}
\begin{definition}[$\lambda$-system]
$L$ lambda system if
\begin{enumerate}
	\item $\Omega \in L$
	\item closed under complements
	\item closed under countable disjoint unions
\end{enumerate}
Alternative definition:
\begin{enumerate}
	\item $\Omega \in L$
	\item $A,B \in L$ and $A\subset B$ then $B\setminus A \in L$
	\item $A_1, A_2,\ldots, \in L$ an increasing sequence of sets, then $\bigcup_i A_i \in L$
\end{enumerate}
\end{definition}
Note that a collection is a $\sigma$-algebra $\iff$ it is both a pi system and a lambda system.  
\begin{theorem}[$\pi-\lambda$ Theorem]
If $P$ is a $\pi$ system and $L$ is a $\lambda$-system with $P\subset L$, then $\sigma(P)\subset L$.\\
Use to proof uniqueness of extension from an algebra to the sigma field.
\end{theorem}
\begin{example}[Quals 2017, Question 2]
\end{example}


\begin{theorem}[Monotone Class Theorem]
Use def:
\begin{definition}[Monotone Class]
$M$ is a monotone class if 
\begin{enumerate}
	\item Closed under increasing unions
	\item Closed under decreasing intersections
\end{enumerate}
\end{definition}
If an algebra $A$ is contained in a monotone class $M$, then $\sigma(A) \subset M$.
\end{theorem}
\begin{theorem}[Monotone Class for functions]
\myworries{Double check this}\\
$M$ be a vector space of $\R$-valued functions on $\Omega$ such that
\begin{enumerate}
	\item $1 \in M$
	\item $M$ is a vector space over $\R$. Ie closed under addition and scalar mult
	\item If $h_n\geq 0\in M$ and $h_n\uparrow h$, then $h\in M$.
\end{enumerate}
Then if $P$ is a pi system such that $\Ind_A \in M$ for all $M\in P$, then $M$ contains all functions that are measurable with respect to $\sigma(P)$. 
\end{theorem}
Ie want to prove something about some functions measurable on $\sigma(P)$... 



\begin{theorem}[Independent $\pi$ systems generate independent sigma algebras]
$$\{C_i\}_{i\in I} \text { independent} \implies \{\sigma(C_i)\} \text{ independent} $$
\begin{example}[Prove two random variables independent]
Check that generating pi systems are independent, ie $P(X\leq b, Y\leq a) = \prod P(X\leq b) P(Y\leq a)$. 
\end{example}

\begin{definition}[Independence of (uncountable) collections of sets]
$\{A_\alpha\}_{\alpha \in I}$ each be a collection of sets. Independent if any \textbf{finite subcollection }are mutually independent. 
\end{definition}

\end{theorem}
\section{Extension theorems}
\begin{theorem}[Caratheodory]
Extend a measure on an algebra to a $\sigma$ algebra, uniquely if finite.\\ \\

The important idea is make an outer measure, then prove the lemma is that the $\mu^*$ measurable sets in $\calF$ for a sigma algebra on which $\mu^*$ is countably additive, ie a bonafide measure. 
\end{theorem}
\section{Random variables}
\begin{definition}[Random variable]
$X: \Omega \to \bbS$ is measurable if:

$$X^{-1}(B):= \{\omega: X(\omega) \in B\} \in \calF \quad \forall B\in \calS$$
Fix an arbitrary set in the co-sigma algebra and check that its preimage is measurable-$\calF$. 
\end{definition}
\begin{recipe}[Measurability]
If $\cal S = \sigma(\cal A)$ and $X^{-1} (A) \in \calF$ for all $A\in \calA$, then $X$ is measurable. \\\\
Note-- nothing necessary about it being a pi system, just pick any convenient generators.
\begin{proof}
	Such sets form a sigma algebra and $\cal S = \sigma(\cal A)$. 
\end{proof}
\end{recipe}


\begin{definition}[Characterize distribution functions of real-valued random variable]
Dembo Theorem 1.2.37. 
\begin{enumerate}
	\item Non decreasing
	\item $\lim_{x\to \infty} F(x) = 1$ and $\to -\infty$ gives $0$
	\item $F$ right continuous  
\end{enumerate}
\end{definition}

\section{0-1 Laws}
\begin{definition}[Tail field]
Defining $\calT_n = \sigma(X_r , r>n)$ and the tail sigma algebra of the process is $\calT = \cap_n \calT_n$.  
\end{definition}
\begin{theorem}[Kolmogorov 0-1]
The tail sigma field is P-trivial. Dembo 1.4.10
\end{theorem}

\section{Borel Cantelli} 
\begin{example}[Longest head runs]
See Durrett. 
\end{example}
A helpful idea is splitting into blocks.\\ \\

\textbf{Truncation} arguments. Want to show something about $\{X_n\}$. Consider:

$$Y_n = X_n \Ind[|X_n| \leq c_n],$$
which will then be integrable. Then prove something about $Y_n$, you can transfer this knowledge to knowledge about $X_n$ by the following idea:

$$P(X_n \neq Y_n \text{ io}) =0 \text { if } \sum_{i=1}^n P(|X_n| >c_n) <\infty$$ 

So if the above is zero, eventually $X_n = Y_n$, so the limiting behavior of $Y_n$ is the same as that of $X_n$. The difficulty is picking a $c_n$ such that the above holds. 

\begin{theorem}[Borel Cantelli 3]
\myworries{Not as useful but still check these}\\
If you have a filtration $\{F_n\}$ and $A_n \in \calF_n$, then:
\begin{enumerate}
	\item ( analog of Borel Cantelli 1)
	$$\sum_{k=1}^\infty P(A_k | \calF_{k-1} )(\omega) <\infty \implies \sum \Ind[\omega \in A_k\} <\infty$$ 
	\item Analog of BC 2
	$$\sum_k P(A_k|\calF_{k-1} )(\omega) = \infty \implies \frac{\sum \Ind[A_k (\omega)]}{\sum P(A_k |\calF_{k-1})(\omega)|} \to 1$$
\end{enumerate}

\end{theorem}

\section{Modes of Convergence}
\begin{recipe}[Proving as convergence]

Some ideas:
\begin{enumerate}
	\item BC 1: check that $P(|X_n - X| > \epsilon \text{ i.o} ) = 0$. Consider \textit{subsequence trick} below. 
	\item If $X_n \conprob X$, then $X_{n_k} \conas X$ for a subsequence. (Good for counter examples) 
	\item $X_n\conprob X$ and $X_n$ is monotone (ie for all $\omega$, $X_n(\omega)$ is increasing in $n$), then $X_n \conas X$.
	\item Continuous mapping theorem 
	\item Skorohod's Representation. If $X_n \implies X$ weakly, then there exists a probability space and random variables $Z_n =^d X_n$ and $Z=^d X$ such that $Z_n \conas Z$. (Good for counter examples) 
\end{enumerate}
\end{recipe}

\begin{theorem}[BC Subsequence trick]
Sometimes $\sum P(A_n) =\infty$. 
\begin{enumerate}
	\item Pick a subsequence such that it's finite. 
	\item Shows that $A_{n_k}$ infinitely often occurs with probability $0$.
	\item Apply interpolation or some other argument for everything in between, ie to conclude that $A_n$ infinitely often wp $0$ also. 
	
\end{enumerate}
Eg for monotone $X_n$:

$$ \frac{X_n}{\E X_n} \leq \frac{X_{n_{k+1} -1}}{\E X_{n_k}}$$
So $\limsup_k \frac{X_{n_{k+1} -1}}{\E X_{n_k}} \leq 1 \implies \limsup_n \frac{X_n}{\E X_n}\leq 1$. We can control the left hand side by controlling the right hand side along our choice of subsequence. 

\myworries{See Dembo notes subsequence section}
Examples-- SLLN, Brownian LLN, LIL. \\ \\

\myworries{SLLN in Persi notes and Renewal theorem in Dembo}

\end{theorem}

\begin{recipe}[Proving almost sure limit is finite]
Try:
\begin{enumerate}
	\item Take expectation - use MCT or something and show that finite, so random variable is finite wp 1
	\item If $\sum_n \Var (X_n) <\infty$ and $\E X_n = 0$ (2.3.17 Dembo) (Kolmogorov 2 series theorem)
	\item Kolmogorov 3 series
\end{enumerate}
\end{recipe}

\begin{recipe}[Proving conv in prob]

Some ideas:
\begin{enumerate}
	\item Show almost sure convergence
	\item Show convergence in $L^p$ for $p\geq 1$ (in $L^2$ in particular useful):
	\begin{itemize}
		\item Show that $\E X_n \to \mu$, $\Var X_n \to 0$, then $X_n \to \mu$ in $L^2$
	\end{itemize}
	\item Show $\Var X_n / b_n^2 \to 0$ and use Markov to get $b_n^{-1} (X_n - \E X_n)\conprob 0$ (and also in $L^2$).
	\item CMT 
	\item $X_n \condist c$ constant
	\item Truncation tool: \textbf{Weak law for Triangular Array}. For when we don't have second moments. 
	\end{enumerate}
\end{recipe}

\begin{theorem}[Coupon Collector]
If $T_n$ is time to get all $n$ possible coupons. Then 

$$T_n/(n\log n) \conprob 1$$ (and in $L^2$)-- see Dembo 2.1.8.
\end{theorem}

\begin{theorem}[Weak law triangular array - 2.1.11 Dembo]
\myworries{Check this}
Suppose triangular array $\{X_{n,k}\}_{{n\in \N, k\leq n}}$ of pairwise independent rv. Define a truncated array with the same \textit{within-row} truncation:
$$\bar X_{n,k} = X_{n,k} \Ind[|X_{n,k}| < b_n ]$$. If we have two conditions:

$$\sum_{k=1}^n P(|X_{n,k}|>b_n) \to^n 0,$$
and,
$$b_n^{-2} \sum_{k=1}^n \Var (\bar X_{n,k}) \to 0,$$
Then:

$$b_n^{-1} (\sum_{k=1}^n X_{n,k} - a_n) \conprob 0 \quad \text{ where } a_n = \sum_{k=1}^n \E \bar X_{n,k}.$$

In case of St Petersburg paradox, then show that $a_n/b_n \to 1/2$ so that we get $b_n^{-1} \sum_{k=1}^n X_k = (b_n^{-1} \sum_{k=1}^n X_{n,k}) \conprob 1/2 $. 



\end{theorem}
\begin{recipe}[Prove $L^p$ convergence]
Ideas - see session notes 
\begin{enumerate}
	\item $X_n \conas X$ and $X_n$ uniformly bounded then $X_n \to^{L1} X$.
	\item If $X_n$ uniformly bounded then $X_n\condist 0 \implies X_n \to^{L1} 0$.
	\item If $|X_n|^p$ is UI then $X_n \condist X \implies \E |X_n|^p \to E|X|^p$ 
	\item Scheffe's Lemma 
\end{enumerate}
\end{recipe}

\begin{recipe}[Proving UI]
Try
\begin{enumerate}
	\item Sums of UI sequence of rvs are UI
	\item Domination: If $E\sup |X_i| < \infty$ then $\{X_i\}$ is UI
	\item If $\{X_i\}$ is UI, then $\sup E |X_i| <\infty$ but not the converse. 
	\item Prop 8.3.3: $X_n$ UI $\iff$ $\sup E|X_i| <\infty$ and for all $\eps$, there exists $\delta$ for all A such that $P(A) <delta$ then $\E[|X_n| \Ind[A] ] <\eps$
	\item If $\sup \E |X_i|^r <\infty$ if $r>1$, then UI 
	\item $X\in L^1$, then $\{\E[X|\calG]: \calG \subset \calF\}$ is UI
	\item $L^1$ convergence implies UI
\end{enumerate}
\end{recipe}

\section{Integration}
\begin{definition}[Lebesgue Integral]
$$\int_\Omega f(\omega) d\mu(\omega) = \sup \left (\sum_{i=1}^n \nu_i \mu(A_i)\right)$$
where $\nu_i = \inf_{\omega \in A_i} f(\omega)$ and the sup is over all partitions of $\Omega$.
\end{definition}
\begin{theorem}[MCT]
Note that can also do for general functions (not necessarily non negative) so long as $f_n \geq g \in L^1$.
\end{theorem}
\begin{theorem}[Fatou's Lemma]
$$\int \lim \inf_n f_n d\mu \leq \lim \inf_n \int f_n d\mu $$
\end{theorem}
\begin{theorem}[DCT]
If $f_n \to f$ a.e $\omega$, and there exists $g$ such that $|f_n(\omega)|\leq g(\omega)$ a.e $\omega$ and $\int gd\mu <\infty$ then exchange integral and limit. Must dominate the sequence. 
\end{theorem}

\begin{theorem}[Scheffe's Lemma]
Is a statement about combining $L^1$ and as convergence. If $f_n \conas f\in L^1$, then:
$$\| f_n -f\|_{L^1} \to 0 \iff \int |f_n| d\mu \to \int |f|d\mu $$
We always have that convergence in $L^1$ implies convergence of the expectations (without any a.s. convergence assumption) , but the other direction is Scheffe's contribution. 
\end{theorem}


%\begin{theorem}[Generalized DCT]
%\myworries{????}
%If $|f_n|\leq g_n$ such that $g_n \to g \in L^1$ (convergence in $L^1$) then $\int f_n d\mu \to \int fd\mu$
%\end{theorem}

\begin{theorem}[Reverse Fatou]
If $f_n \leq g \in L^1$ then :

$$\lim \sup_n \int f_n d\mu \leq \int \lim \sup f_n d\mu$$
\end{theorem}

\begin{fact}[$L^p$ spaces nested]
$$\|Y\|_r \leq \|Y\|_q$$
\end{fact}

\begin{fact}[$L^q$ convergence fact - Dembo 1.3.28]
$X_n\to^{L^q} X \implies \E|X_n|^q \to \E|X_\infty|^q$ for any $q$. (Minkowski)\\\\ Also for only $q\in \N$, $\E X_n^q \to \E X_\infty^q$. (some wild algebraic shit for odd $q$).

\end{fact}
\begin{theorem}[Holder's]
If $p,q>1$ with $1/p + 1/q = 1$ then 
$$E|XY| \leq \|X\|_p \|Y\|_q $$
Cauchy Schwarz is special case.
\end{theorem}

\begin{theorem}[Minkowski]
Triangle inequality for the $\|\cdot \|_p $ norm
\end{theorem}

\begin{definition}[Uniform integrability (UI)]
Possibly uncountable collection $\{X_\alpha: \alpha \in I\}$ is called UI if 

$$\lim_{M\to\infty} \sup_{\alpha \in I} \E[|X_\alpha| \Ind[|X_\alpha | >M] = 0$$
\end{definition}
\begin{fact}[Dominated implies UI]
If $|X_\alpha|\leq Y$ for integrable Y, then collection is UI. \\\\

As a corollary, any finite collection of integrable rv is UI.
\end{fact}

\begin{theorem}[Vitali Convergence Theorem]
Supposing that $X_n \conprob X$, then:
$$\{X_n\} \text{ is UI} \iff X_n \to^{L1} X \iff X_n \text{ is integrable for all } n\leq \infty \text{ and } E|X_n| \to E|X_\infty|.$$
\end{theorem}




\section{Product $\sigma$-algebras}
Existence of unique product measure of $n$ $\sigma-$ finite measures. \\\\

\begin{theorem}[Kolmogorov Extension]
Unique probability measure on $(\R^\N, \calB_c)$ with correct FDDs. 
\end{theorem}



\begin{theorem}[Fubini's]
Conditions: $h\geq0$ or $\int |h| d\mu <\infty $ where $\mu = \mu_1 \times \mu_2$. 
\end{theorem}
%% Weak Convergence
\section{Weak Convergence}
\subsection{Methods}
Main tools: 

\begin{enumerate}
	\item 	\textbf{\textbf{Direct}}. Show that $F_n(x) \to F(x)$ for all continuity points.
	\item \textbf{Characteristic functions}- show that $\phi_n(t) \to \phi(t)$ for all $t\in \R$. ($\phi$ is a characteristic function of a probability measure if $\psi(t) \to 1$ as $t\downarrow 0$. 
	\item \textbf{CLT} 

\end{enumerate}

\begin{example}[Cycling of Random Number Generators (2007 Q2)]
Similar to birthday problem - direct method. 
\answer{

\begin{align}
	P(T>k) & = \prod_{i=1}^k (1-\frac{i}{n})\\
	&\approx \prod \exp(-1/n)\\
	& \approx \exp(-k^2/n).
\end{align}
So $P(T> x\sqrt{n}) \approx \exp(-x^2/2)$ should work.\\\\
Need to justify this rigorously. To do so, use $|\log (1+x) - x| < Cx^2$ when $|x| <1/2$. Ie, $\log (1+x) = x + O(x^2)$. \\\\ 

See lecture 1 in 310a. 

}
\end{example}

Backup plans

\begin{enumerate}
	\item If there's a density, try to show that $f_n(x) \to f_\infty$ and check that $f_\infty$ a valid pdf. 
	\item If $X_n \geq 0$, show that $\int_0^\infty \exp(-\lambda x) d\mu_n (x) \to L(\lambda) = \int_0^\infty \exp(-\lambda x) d\mu_\infty (x)$. Note that $L(\lambda)$ is a Laplace transform of some $\mu$ if $L(\lambda)\downarrow 1$ as $\lambda\downarrow 0$. (Just need for positive $\lambda$. 
	\item MGFs
	\item Moment Method
	\item Can you use a Prohorov argument?
\end{enumerate}

\begin{theorem}[Moment Method]
If can show that $\int X^k \mu_n (dx) \to \int X^k \mu_\infty (dx)$ and $\mu_\infty$ is the only measure with its particular moments, then $\mu_n\condist \mu$. \\\\
To show the second thing, the following are sufficient:
\begin{enumerate}
	\item Mgf exists
	\item \myworries{Power series condition?}
	\item Carleman's condition: $\sum_{k\geq 1} (m_{2k})^{-1/2k} <\infty$
\end{enumerate}


\begin{example}[(Non-)Examples defined by moments]
Examples: Normal, poisson, exponential. \\ \\

Non-examples: Log-normal
\end{example}
\end{theorem}
\begin{theorem}[Convergence of type]
If two normalizing sequences, then those sequences are basically the same up to some scaling.\\\\
Idea is you can't change the class of limniting distribution by rescaling. 
\end{theorem}

%%%%% CLT %%%%%%
\section{CLT}
Heuristic: "not too dependent", "no few terms dominate". 
\begin{theorem}[Lindeberg CLT]
Suppose we have a triangular array such that:

\begin{enumerate}
	\item  for fixed $n$, $\{X_{ni}\}_{i=1}^{k_n}$ are independent (ie independence within row). 
	\item Suppose also $\E (X_{ni}) = 0$ for all, and $\Var X_{ni} = \sigma^2_{ni} <\infty$.
	\item Define $S_n = \sum_{i=1}^{k_n} X_{ni}$ and $s_n^2 = \sum_{i=1}^{k_n} \sigma^2_{ni}$ (sum of rows)
	\item \textbf{Lindeberg condition} holds ie for all $\eps>0$:
	$$\lim_{n\to \infty} \frac{1}{s_n^2} \sum_{i=1}^{k_n} \int X_{ni}^2 \Ind[ |X_{ni} | >\epsilon s_n ] dP = 0$$
\end{enumerate}  

Then:

$$\frac{S_n}{s_n} \condist \calN(0,1).$$

Note that if not mean zero, subtract off the means:

$$\frac{S_n - \sum_{i=1}^{k_n} \mu_{kn}}{s_n} \condist N(0,1)$$

\end{theorem}

\begin{example}[2007 Problem 1]
$X_k$ takes values $\pm k^a$ wp $\frac{1}{2} k^{-\alpha}$ each and $\pm 1$ wp $\frac{1}{2} (1-k^{-a})$ each. Ie Rademacher plus something big. Want to know if $S_n/c_n$ is Gaussian. \\\\

For $a>1$, apply BC + CLT on iid Rademachers to get a normal limit.\\\\

For $a<1$, try Lindeberg:


$$\Var X_k \sim \frac{k^{2a}}{k^a} \implies \sigma_n^2 \sim 2 \sum_{k=1}^n k^a \sim n^{a+1}$$

And apply Lindeberg. \\\\

For $a=1$ case use CF. 


\end{example}

\begin{example}[CLT failures: too wild]
$$X_i = \begin{cases}
	0 \text{ wp } 1-1/i\\
		1 \text{ wp } 1/i
		\end{cases}$$
		
The issue is that some $X_i$'s  dominate-- ie the big ones.
\end{example}

\begin{example}[CLT Failures: too dependent]
\end{example}


\begin{recipe}[CLT for non-square integrable]
Session 4 notes. Similar to convergence in probability strategy. \\\\
Assume $X_{n,k} \in L^1$ and exist $c_n$ such that 

\begin{enumerate}
	\item $\sum_{k=1}^{\ell_n} P(|X_{n,k}| > c_n) = o(1)$
	\item Lindeberg condition satisfied for truncated $Y_{n,k} =X_{n,k} \Ind [X_{n,k} \leq c_n]$
	\item $\sum_{k=1}^{\ell_n} (\E X_{n,k} - \E Y_{n,k}) = o(s_n)$ where $s_n^2$ sum of variances of truncated in the $n$-th row.
\end{enumerate}
Then $$\frac{\sum_{k=1}^{\ell_n} X_{n,k} - \E X_{n,k}}{s_n} \condist N(0,1)$$

For example, see Dembo 3.1.12.
\end{recipe}
\begin{theorem}[Lyapunov CLT]
Lyapunov condition is sufficient for Lindeberg's condition. Same setup, check that:


$$s_n^{-2-\delta} \sum_{i=1}^{k_n} \E |X_{n,k} - \E (X_{n,k})|^{2+\delta} \to 0 \quad \text{for some } \delta > 0$$
\end{theorem}


\begin{theorem}[Other CLTs for dependent rvs]
Look at Sourav/Quals notes
\end{theorem}
\begin{theorem}[Kolmogorov 3 Series Theorem]
....
\end{theorem}

Some helpful CLT references:
\begin{enumerate}
	\item Exponential approximation for the geometric - pg 105 dembo
	\item Normal approx to Poisson - Dembo ch 3
	\item Normal approx to Binomial
\end{enumerate}

Some limit theorems for max of random variables (Ex 3.2.13 Dembo). The below three are the only possible type of limits for max of iid random variables. 
\begin{enumerate}
	\item Max of exponentials or normals is is Gumbel-type $F_\infty(y) = \exp(-e^{-y})$. (see also 3.2.14)
	\item Frechet type $F_\infty(y) = \exp(-y^{-\alpha})$
	\item Weibull type $F_\infty(y) = \exp(-|y|^\alpha)$
\end{enumerate}
A few more examples of max of rvs and limiting distributions

\begin{example}[Birthday problem limiting law]
$T_n$ is the number needed to get a match with $n$ possible birthdays (eg $n=365$), then can show that $P(n^{-1/2} T_n >s) \to \exp(-s^2/2)$ - Ex 3.2.15. See also the RNG 2007 Q2 question below. 
\end{example}

%%% Characteristic function %%


\section{Characteristic Functions}
\begin{definition}[Characteristic Function]
Characteristic function is Fourier transform of $\mu$. 

$$\phi(t) = \E [\exp(itX)].$$


\begin{recipe}[Characteristic Function Chaos]
Try to get $\phi(t) \approx (1+ \frac{f(t)}{n})^n$ form so that we can use exp limit. 
\end{recipe}
\end{definition}

\begin{recipe}[Characteristic Function]
Given a triangular array, want to show weak convergence of $\sum_{k=1}^{\ell_n} X_{n,k}$. 
\begin{enumerate}
	\item Center the random variables if necessary
	\item Try to justify approximation:
	$$\log \prod_{k=1}^{\ell_n} \phi_{n,k} (t) \approx -\sum_{k=1}^{\ell_n} (1-\phi_{n,k}(t))$$
	\item Use Lemma: Dembo 3.3.31, Chaterjee 8.10.4 which says: \\\\
	If $a_1,\ldots, a_n$ and $b_1,\ldots b_n$ complex with maximum modulus $1$ then:
	$$|\prod_{j=1}^n a_j - \prod_{j=1}^n b_j| \leq \sum_{j=1}^n |a_j - b_j|$$
	Then since Taylor Approx gives $|\exp(z) - 1-z| \leq C|z|^2 $ for all small $|z|$, then 
	$$|\prod_{k=1}^{\ell_n} \phi_{n,k} (t) - \exp\left[ -\sum_{k=1}^{\ell_n} (1-\phi_{n,k}(t)\right]| \leq \sum_{k=1}^{\ell_n} |\phi_{n,k}(t) - \exp(-[1-\phi_{n,k}(t)])| \leq C\sum_{k=1}^{\ell_n} |1-\phi_{n,k}(t)|^2 $$
	Want to show that this goes to $0$. Which we can do by showing:
	$$\sum_{k=1}^{\ell_n} |1-\phi_{n,k}(t)| = O_n(1) \quad \max_{k\leq \ell_n} (1-\phi_{n,k}(t)) = o_n(1)$$ 
	So that we can show the $C\sum | \cdot |^2$ term goes to $0$.
	\item \myworries{See tim notes..... }
	\item ....
	\item Apply Levy Continuity Theorem 
\end{enumerate}
\end{recipe}


\begin{example}[Cauchy Examples 2017 Quals]
If can show that 

$$\sum_{k=1}^n 1-{\phi_{1/X_i}}(t_n) \to -c|t|$$

Because of iid case we're good
\end{example}
\begin{theorem}[Levy's continuity Theorem]
If $\phi_{\mu_n}(t) \to \phi(t)$ pointwise and $\phi$ is continuous at $t=0$, then $\mu_n$ is uniformly tight sequence and $\mu_n \implies \mu$ weakly where characteristic function of $\mu$ given by $\phi$.\\ \\

Conversely, weak convergence implies convergence of characteristic functions.
\end{theorem}



\section{Stein's Method (Poisson)}


Tool for weak convergence for Poisson approximation. Also for Gaussian approximation but we didn't cover it in class. \\\\
\textbf{Gaussian heuristic:} sums of weakly dependent random variables of roughly the same size. \\\\
\textbf{Poisson heuristic:} number of occurrences of rare events which are weakly dependent. \\\\
Idea behind Stein's method:

$$X \sim N(\mu,\sigma^2) \iff \forall \text{ nice } f, \E f'(X) = \E X f(X)$$
So we define this Stein operator -- $\calA f(x) = f'(x) - xf(x)$, then $\E \mathcal A (f(X)) = 0$. Then vibes is that if $\calA f(X) \approx 0$, then $X\approx \text{Gaussian}$. Stein quantifies this. \\\\

%For Poisson, the characterizing operator is $\calA f(k) = \lambda f(k+1) - kf(k)$ for all functions $\bbZ_{\geq 0} \to \R$. Stein's equation for Poisson is:
%
%$$\lambda f_A (n+1) - nf_A(n) = \Ind_A(n) - P(\operatorname{Pois}(\lambda) \in A),$$  
%want a solution $f_A$. There exists bounded solution $f_A$ such that $\sup_{n\geq 1} |f_A(n+1) - f_A(n)| \leq 1 \wedge \lambda^{-1}$. Yields:
%
%$$d_{TV} (Y, \operatorname{Pois}(\lambda)) \leq \sup \{ |\lambda \E f(Y+1) - \E Y f(Y) |: f\text{ satisfiying above}\}$$



Index set examples. The indicators should be rare. 
\begin{enumerate}
	\item Coupon collector: indicator that a coupon is \textbf{not} present. $|I| = n$.
	\item ER Graph: indicator that three nodes form a triangle, $|I| = \binom{n}{3}$.
	\item 
\end{enumerate}



\subsection{Method 1 - Dependency Graphs}

Define $N_i \subset I$ such that $i \in N_{i} \implies X_i \indep \{X_j: j\notin N_{i} \} $. (Ie a dependency graph - if it's not in the neighborhood, they're independent). 

\[
\left\|P_{W}-\operatorname{Poisson}(\lambda)\right\|_{\mathrm{TV}}
\;\le\;
\min\!\bigl(3,\lambda^{-1}\bigr)\,
\biggl[
   \sum_{i\in I}\sum_{j\in N_i\setminus\{i\}} p_{ij}
   \;+\;
   \sum_{i\in I}\sum_{j\in N_i} p_i p_j
\biggr].
\]

\begin{recipe}[Dependency Graphs]
If you can make a dependency graph that isn't complete, this is a good strat. 
\begin{enumerate}
	\item Write $W = \sum_{i\in I} X_i$ where $X_i$ are indicators of some events. 
	\item Calculate $p_i$, $\lambda$.
	\item How are the indicators correlated? Form a dependency graph. 
	\item Show the bound goes to $0$. 
\end{enumerate}
\end{recipe}

\begin{example}[Session 6- ER Graph Triangles]
$|I| = \binom{n}{3}$
\end{example}
\begin{example}[Session 6 - Head Runs]
Consider $n$ coin tosses with probability of heads $p$. $Y_n = \#$ head runs of at least $k$. Define $X_i = \Ind[ \text{exists a head run of length at least k starting at i}]$. $Y_n = \sum_{i=1}^n X_i $. 

$$P(X_i = 1) = \begin{cases}
	p^k \quad i = 1\\
	(1-p)p^k \quad i\in [2, n-k +1]\\
	0 \quad i> n-k+1 
\end{cases}.$$

$X_i$ and $X_j$ are independent if we look far enough away, ie whenever $|i - j| > k$ 

\end{example}


\subsection{Method 2 - Positive ASsociations}
Suppose $Y = \sum_{i\in I} X_i$. 
\myworries{Check this}

\subsection{Method 3 - Negative Association - More useful.}
For all $i \in I$, we can construct $\{Y^{(i)}_j : j \neq i\}$ coupled with $X_i$ such that
\[
  \{Y^{(i)}_j : j \neq i\} 
  \stackrel{d}{=}
  \{X_j : j \neq i\} \mid X_i = 1
  \quad\text{and}\quad
  Y^{(i)}_j \le X_j \;\; \forall\, j \neq i .
\]

In this case,
\[
  d_{\mathrm{TV}}\!\bigl(Y, \operatorname{Poisson}(\lambda)\bigr)
  \;\le\;
  \bigl(1 \wedge \lambda^{-1}\bigr)\,
  \bigl(\lambda - \operatorname{Var}(Y)\bigr).
\]

\begin{example}[Coupon Collector]
Number of missing coupons after many trials is approximately Poisson. \\\\
Isomorphic to balls into bins. N bins (N coupons). Then b balls - number of trials picking a new coupon. \\\\

$$X_i = \Ind[\textbf{box i empty}] \text{ are rare events}.$$
Moreover, they're negatively associated-- if one box is empty, then the other boxes are more likely to be non-empty. 
\end{example}

\begin{example}[Baseball question - 2015, Q4 ]
5 cards per pack, each pack has unique cards randomly. 600 total players. How many packs do we need? This is like the coupon collector problem. 
For $p$ packs, 
$$p_i = (1- 1/120)^p \approx \exp(-p/120).$$
So $W\approx Pois(\lambda)$ with $\lambda = 600\exp(-p/120)$. 
Find $p$ such that
$$P(W = 0) = 0.95).$$\\\\
Justifying the approximation. Negative association. Other players more likely to have been chosen if one player is missing. By exchangeability/symmetry: check that the negative association condition holds for $i=1$. Ie need to show that there exists a suitable coupling. Construct as follows. Amongst all $p$ draws, if I get a pack with player $1$ in it, resample another player to substitute out player $1$ (do it so that the pack is still distinct). Then:

$$Y_{j}^{(1)} = \Ind[ \text{ player j is not in the "resampled" p packs defined above}].$$
Since whenever we get a player $1$, we resample, it forces $Y_{j}^{(1)} \leq X_j$ for all $j\neq i$. \myworries{CHECK THIS LATER}

$$\Var Y = \sum \Var X_i + 2\sum_{i<j} \Cov(X_i, X_j) = \sum p_i(1-p_i) + 2\sum (\Cov (X_i, X_j))$$

So tv distance is bounded above by:

$$(1\wedge \lambda^{-1} ) [\sum p_i^2 - 2\sum \Cov (X_i, X_j)]$$
\end{example}

\subsection{Other Arratia Goldstein Method}

\begin{example}[Fixed Points - 310a HW8]
    Let $\sigma$ be a uniformly chosen permutation in the symmetric group $S_n$. Let $W = \#\{i : \sigma(i) = i\}$ (the number of fixed points in $\sigma$). Show that $W$ has an approximate Poisson(1) distribution by using Stein's method to get an upper bound on $\|P_W - \text{Poisson}(1)\|$. (Hint: see section 4.5 of Arratia-Goldstein-Gordon.) Give details for this specific case.

\answer{
Let $I = [n]$. We choose $B_\alpha = \{\alpha\}$ and use Theorem 1 from Arratia-Goldstein-Gordon. 

For each $i\in I$, letasdf

$$X_i = \begin{cases}
    1 \text{ if } \sigma(i) = i\\
    0 \text{ otherwise}
\end{cases}.$$
Naturally, $P(X_i =1) = \frac{1}{n}$. We let $W = \sum_{i\in I} X_i$ and $\lambda = E[W] = 1$. We now use Stein's method as given in Arratia-Goldstein-Gordon Theorem 1 to get an upper bound on $\lVert P_W - Pois(1)\rVert$.
$$b_1 = \sum_{\alpha\in I} \sum_{\beta \in B_\alpha} p_\alpha p_\beta = \sum_{\alpha \in I} p_\alpha^2 = \frac{1}{n}.$$
Next, because we let $B_\alpha = \{\alpha\}$, 
$$b_2 = 0.$$
Finally, for the third term, by Lemma 2 (p 418) in Arratia et al,
$$b_3 \leq \min_{1<k<n} (\frac{2k}{n-k} + 2n 2^{-k} e^{e})\sim 2\frac{(2log_2 n + e/ln 2)}{n},$$
due to the fact that $\lambda=1$ in our problem, so $\lambda = o(n)$

Now note that as $n\to \infty$, $b_1\to 0$ and $b_3\to 0$, so, noting that the Arratia paper's definition of TV distance is twice our definition of TV distance:

$$\lVert P_W - Pois(1)\rVert\leq b_1 + b_2 + b_3 = \frac{1}{n} + \frac{4\log_2 n + 2\epsilon/\ln 2}{n} + o(1)$$
Now as $n\to \infty$, $b_1 \to 0$ and $b_3 \to 0$, so $\lVert P_W - Pois(1)\rVert \to 0$. 
}
\end{example}

\begin{example}[Near Fixed Points- 2004 Q2]
    
\end{example}



\newpage 
\section{Approximations}

$$1-x \leq e^{-x} \quad 1-x\geq e^{-2x} \quad \text{both for small } x?$$  
$$\log(1+x) = x + O(x^2)) \quad \text{ for small } x$$

$$-x-x^2 \leq \log(1-x) \leq -x \quad \text{for } x\in [0,1/2].$$



\subsection{Binomial Coeffs and Stirlings}
$$(\frac{n}{k})^k \leq \binom{n}{k} \leq (\frac{ne}{k})^k $$
\myworries{Stirlings}
\begin{theorem}[Stirlings]
$$(1-\epsilon) \sqrt{2\pi} k^{k+1/2}e^{-k}\leq k! \leq (1+\epsilon) \sqrt{2\pi} k^{k+1/2}e^{-k}$$
\end{theorem}


\section{TV Distance}

$$d_{TV} (P, Q) = \sup_B |P(B) - Q(B)| = \frac{1}{2} \int |p(x) - q(x)| dx = 1- \int \min\{ p(x), q(x)\} dx $$ 

\section{Misc}
\begin{definition}[Metric]
\end{definition}

\subsection{Series convergence}
\begin{theorem}[Root Test]
If $S_n = \sum_{k=1}^n a_n$ then $S_n$ converges if $L = \limsup_n |a_n|^{1/n} <1$. If $=1$, no info. $>1$, diverges. 
\end{theorem}


\begin{theorem}[Kolmogorov's Maximal inequality]
$Y_i$ mutually independent with $\E Y_i^2 <\infty$ and $\E Y_i = 0$, then for any $z>0$:

$$z^2 P(\max_{k\leq n} |S_k| \geq z) \leq \Var (Z_n)$$
where $S_n = \sum_{k=1}^n Y_k$.
\end{theorem}
\newpage





\end{document}