\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{hw}
\usepackage{tcolorbox}
\usepackage[ruled,vlined]{algorithm2e}

% ------------------  Bibliography  ------------------
%\usepackage[
%  backend=biber,
%  style=numeric,      % plain numbers: [1]
%  sorting=none,       % keep refs in citation order
%  maxbibnames=99      % show all authors in bib
%]{biblatex}
\usepackage{booktabs}
% ----------------------------------------------------

\tcbuselibrary{breakable}
\title{300 Quals Guide}
\newcommand{\answer}[1]{%
  \begin{tcolorbox}[
    colback=gray!9.8,
    boxrule=0.5pt,
    breakable]
  \small #1
  \end{tcolorbox}}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\newcommand{\simiid}{\overset{iid}\sim }
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\row}{\operatorname{row}}
\newcommand{\Proj}{\operatorname{Proj}}
\newcommand{\Unif}{\operatorname{Unif}}

\begin{document}
\maketitle
\section{Misc facts}
\begin{definition}[Convexity]
For all $0\leq t\leq 1$ and $x_1,x_2 \in X$:
$$f(tx_1 + (1-t)x_2) \leq tf(x_1) + (1-t) f(x_2).$$
Alternative, check the second derivative $\geq 0$
\end{definition}
\begin{theorem}[Jensen's Inequality]
If $\phi$ convex, then $\E \phi(X) \geq \phi(EX)$. Eg $EX^2 \geq (EX)^2$. Inequality is strict if $\phi$ is strictly convex and $X$ is not degenerate (constant). Also conditional version holds. 
\end{theorem}
\section{Exponential Families}
\begin{definition}[Exponential Family]
	$$p_\theta(x) = \exp(\sum_{i=1}^s T_i(X) \eta_i(\theta) - A(\theta))h(x)$$
	$$p_\eta(x) = \exp(\sum_{i=1}^s T_i(X) \eta_i - \tilde A(\eta)) h(x)$$
\end{definition}
\begin{fact}[Expectation and covariance of sufficient stats in exponential fam]
$$\E[T(X)] = \grad A(\eta)$$
$$\Cov (T_i(X), T_j(X)) = \partial_{\eta_i} \partial_{\eta_j} A(\eta)$$
\end{fact}

\subsection{All the Expo family examples}

Include curved.. 


\section{Sufficiency} 
"Throwing away everything else besides this statistic entails no loss of info in estimating $\theta$"
\begin{definition}[Sufficiency]
If for all $t$, the distribution of $X|T=t$ does not depend on $\theta$.
\end{definition}
If you collect data $X_1,\ldots,X_n$, then throw away data except for $T=t$, then can construct $\tilde X_1,\ldots \tilde X_n$ with same distribution as $\underline{X}$ just by knowing $T$.

\myworries{Make a table of good examples including $\Unif(0,\theta)$}
\begin{theorem}[Factorization Theorem]
$T$ sufficient for $\theta$ (or for the model) $\iff$ we can write $p_\theta(\underline{x}) = g_\theta(t(\underline{x})) h(\underline{x})$. Ie, can factor the density into a part that depends on statistic and $\theta$, and another part that depends on data but not $\theta$. 
\begin{example}[Expo($\theta$)]
Can write $p_\theta(\underline x) = \theta^n \exp(-\theta\sum x_i)$ so $T(X) = \sum_{i=1}^n X_i$ is sufficient.
\end{example}
\begin{example}[Exponential families]
Look at the form -- $T$ is sufficient.
\end{example}


\end{theorem}
\begin{definition}[Minimal sufficiency]
For all $T'$ sufficient, $T$ is a function of $T'$.
\end{definition}


\begin{theorem}[Rao-Blackwell]
If $T$ sufficient, $L$ is any convex loss, and $\delta$ estimates $g(\theta)$, define:

$$\eta(T) = \E[\delta(X)|T],$$
then using Tower property and Jensen's:

$$R(\theta, \delta(X) = \E[g(\theta), \delta(X)] \geq \E [L(\theta, \eta(T)) = R(\theta, \eta(T))$$
\end{theorem}
\begin{definition}[Ancillary Statistic]
Distribution of $A(X)$ does not depend on $\theta$.
\begin{definition}[First Order Ancillary]
If $\E_\theta A(X)$ does not depend on $\theta$. Weaker than Ancillary. 
\end{definition}
\end{definition}
\section{Completeness} 
\begin{definition}[Completeness]
$T(X)$ complete if no non-constant function of $T$ is even 1st order ancillary, or equivalently:

$$\E f(T) = 0 \implies f(T) = 0 \text{ a.s}.$$
\end{definition}
\begin{example}[$X_i\sim\Unif(0,\theta)$]
See coaching notes-- break $h$ into positive and negative parts to show $h = 0$ as. 	
\end{example}
\begin{theorem}[Full rank exponential family]
Ie if $(\eta_1,\ldots, \eta_k): \eta \in \Omega$ contains a $k-$dimensional rectangle, then $T$ is not only sufficient, but \textbf{also complete}.
\end{theorem}
\begin{theorem}[Basu's Theorem]
If $T$ is CSS and $A$ Ancillary, then $T\indep A$. 
\end{theorem}
A common strategy with Basu's theorem is to consider a submodel-- show independence in the submodel, then based on arbitrariness of submodel, show true for full model. 
\begin{example}[Show sample mean and sample variance in $N(\mu,\sigma^2)$ are independent]
(Assume both unknown). then in the submodel where $\sigma = \sigma_0$ known, then $\bar X$ is CSS. Can show that $\sum (X_i - \bar X)^2$ is Ancillary. So $\bar X \indep \sum (X_i - \bar X)^2$. This is true for all $\mu$, and for $\sigma = \sigma_0$ fixed. But $\sigma_0$ arbtirary, so true for all $\mu,\sigma$. 	
\end{example}
\myworries{Add a table here including 2 param Unif}
\begin{enumerate}
	\item $\Unif(0,\theta)$ : $T= X_{(n)}$
	\item $\Unif(\theta_1, \theta_2)$: $T = (X_{(1)}, X_{(n)})$
\end{enumerate}

\section{UMVU}
Note that we can't take existence of an unbiased estimator for granted. If $x\sim Bin(n,\theta)$. Take like $g(\theta) = \frac{1-\theta}{\theta}$ or a polynomial of degree $>n$-- no unbiased estimators. Strategy to show no unbiased estimator is just write out the expectation of an estimator.
\begin{definition}[UMVU]
$\delta^*$ is unbiased and for all other $\delta$, $R(\delta^*,g(\theta)) \leq R(\delta,g(\theta))$ for all $\theta$.
\end{definition}
Note that if we have an unbiased estimator, we can Rao-Blackwellize with a sufficient statistic and still have unbiased estimator. 
\begin{theorem}[Lehman-Scheffe]
If unbiased estimator is function of CSS, it is UMVU. (Since there exists at most one unbiased estimator that's function of $T$ by completeness).\\\\
Alternate statement: if there exists any unbiased estimator and a CSS $T$, then there is a unique unbiased estimator that's a function of $T$, which is UMVU (UMRU for any convex loss). Unique UMRU if strict convex loss, since this makes Jensen strict. 

\end{theorem}
\begin{recipe}[UMVU with CSS]Steps:
	\begin{enumerate}
	\item Find a CSS
	\item Find an unbiased estimator function of CSS. (Alternatively, find a dumb unbiased estimator and RB)
	\item $\implies$ UMVU
\end{enumerate}
\end{recipe}
Note that UMVU can be inadmissible - see James-Stein or the Poisson UMVU for $g(\theta).= \exp(-3\lambda)$ example in Lecture 5. 
\begin{theorem}[Orthogonality Condition]
Let $\hat \theta$ be an unbiased estimator with $E_P \hat\theta^2 <\infty$ if $U(X)$ is an unbiased estimator of $0$ for all $P$, then:

$$\hat \theta \text { UMVU } \iff E_P [\hat \theta(X) U(X) ] = 0 \text { for all unbiased estimators of } 0 \text{ and for all P}$$
An application is showing that the addition of UMVUs is UMVU. Also, product of UMVUs is UMVU for its expectation. \\
Also reasonable to try to show not UMVU. 
\myworries{Come back to Ex 2.3 in Notes}
\end{theorem}
\begin{theorem}[Cramer Rao LB]
For any $\hat \theta$ unbiased for $\theta$:

$$\Var_\theta \hat \theta \succ I_\theta^{-1}$$
when $\{P_\theta\}$ is QMD with non singular $I_\theta$. 
\end{theorem}

\subsection{UMVU Examples}
\begin{enumerate}
	\item Basic Expo Fam examples -- eg $\bar X$ in Bernouli or Normal with known variance 
	\item Empirical CDF in $\mathcal N(\theta,1)$.
	\begin{itemize}
		\item CSS is $\bar X$, $\delta = \Ind[X_1 < u]$ unbiased. 
		\item Idea to RB then add and subtract $\bar X$ since $X_1 - \bar X$ is ancillary, then apply Basu
		\item UMVU is $\Phi(\frac{u - \bar X}{\sqrt{(n-1)/n}})$
	\end{itemize}

\end{enumerate}
Non parametric examples
\begin{enumerate}
	\item $X_i \sim F \in \calF = \{ \text{ all distributions with density wrt Lebesgue and finite variance } \}$. $g(\theta) = E_F X_i$.
	\begin{itemize}
		\item Note that $\bar X$ is unbiased in the big family and is UMVU in the normal \textbf{subfamily} 
		\item Order statistics CSS (always sufficient - complete by subfamily arg and bijection with sums of powers): $(X_{(1)}, \ldots, X_{(n)}) \iff (\sum X_i, \ldots, \sum X_i^n)$ bijection. 
		\item So $\bar X$ is UMVU
	\end{itemize}
	\item $X_i$ iid symmetric about $\theta$, $E X_i = \theta$. Finite variance. 
	\begin{itemize}
		\item Two subclasses: normal family, $\Unif(\theta_1, \theta_2)$ family - have different UMVUs and both are unbiased in the original class
	\end{itemize}
\end{enumerate}
\begin{recipe}[Subfamily UMVU Argument]
If UMVU in subfamily and unbiased in big family, must be UMVU in big family \textbf{if} an  UMVU exists in the big family. Because the UMVU \textbf{uniquely} minimizes variance in the subfamily. 
\begin{proof}
	Take the big family. Take a function such that $E_\theta f(X) = 0$ for all $\theta$. Then true for subfamily. So $P_\theta (f(X) = 0)$ for all $\theta$ in the subfamily. Same null sets as big family, means that also $P_\theta (f(X) = 0)$ for the big family.
\end{proof}
\end{recipe}
\begin{fact}[Completeness in subfamily]
If $\calF_0 \subset \calF$ and they have the same null sets, then completeness in $\calF_0$ implies completeness in $\calF$. 
\end{fact}
\begin{recipe}[Non-existence of Non-parametric UMVU]
Find two different subclasses with different unique UMVU that are also unbiased in the big class -- no UMVU in big class.	
\end{recipe}
If we can't use Lehman-Scheffe, try one of the following ideas:
\begin{enumerate}
	\item Cramer-Rao lower bound
	\item Orthogonality condition
	\item Subfamily arguments
\end{enumerate}
\subsection{Non-convex loss functions}
If loss is bounded, there is no UMRU estimator. This is unbiased:
$$\delta_\pi = \begin{cases}
	g(\theta_0) \text{ wp } 1-\pi\\
	\frac 1 \pi [\delta_0 (X) - g(\theta_0)] + g(\theta_0)
\end{cases}$$
and its risk is $\pi M$, so it could be arbitrarily small.



\section{MRE}
Big picture-- there's an easy recipe for MRE for square error or absolute error if we pick any old $\delta_0$ equivariant function of CSS. 

\begin{definition}[Location models]
Density satisfies
$$f_{\theta + h} (x+h) = f_\theta(x)$$
Think: normal at its mean has same density as a shifted normal at shifted mean.\\ 
A \textbf{location invariant loss} is $\ell(a + h,\theta + h ) = \ell(a,\theta)$ for all $a, \theta, h$. 
\end{definition}
Want location equivariant estimators, ie $\hat \theta(x + h) = \hat(\theta) + h$ -- if our observations all shift by some amount, our estimate of the location should also shift by that amount. 

\begin{fact}[Bias, variance, risk of equivariant estimators]
Do not depend on $\theta$. Ie, 
$$\E_\theta \hat \theta = \theta + b \text{ for all } \theta $$
for risk, since it's constant, we can hope to find the best among all equivariant estimators.
\end{fact}

\begin{definition}[MRE]
Satisfies equivariance condition: $$\hat \theta(x+c, u) = \hat \theta (x,u) + c$$
and minimum risk condition amongst all equivariant estimators. 
\end{definition}
\begin{definition}[Characterization of location \textbf{invariant} estimators]
Location invariant means $U(X_1 + c,\ldots,X_n + c) = U(X)$. Characterize by:
$U$ location invariant $\iff U = V(y_1, \ldots, y_{n-1})$ where $y_i = X_i - X_n$. 
\end{definition}
\begin{fact}[Characterization of location \textbf{equivariant} estimators]
Let $\delta_0$ be \textbf{any} location equivariant estimator, eg $\delta_0 = \bar X$. 
$$\delta \text { is location equivariant} \iff \delta(X_1,\ldots,X_n) = \delta_0(X_1,\ldots,X_n) + U(X_1,\ldots, X_n) $$ 
where $U$ is location \textbf{invariant}. Then from invariant characterization:
$$\iff \delta(X_1,\ldots,X_n) = \delta_0(X_1,\ldots,X_n) + V(Y_1,\ldots, Y_{n-1})$$
\end{fact}

\begin{recipe}[Finding the MRE]
Let $X_i$ observations iid from location model and let $Y = (X_1 - X_n, \ldots, X_{n-1} - X_n)$. Let $\hat \theta_0$ be any location equivariant estimator of $\theta_0$ with finite risk. If the following is well-defined:
$$v(y) = \arg \min _v E_0 [\ell(\hat \theta_0 (X) - v, 0) \mid Y = y]$$
Then there exists an MRE $\hat \theta^*(X) = \hat \theta_0(X) - v(Y)$.	
\end{recipe}

Want to minimize 
	$$\E_\theta [ \rho (\delta_0(X)- V(Y) - \theta)] = \E_0 \rho (\delta_0(X) - V(Y))$$
	Apply Tower property and minimize the inner expectation. For square error yields:
	$$V = \E_0 [\delta_0 (X) |Y)]$$
	So $\delta^* = \delta_0 - E_0 [\delta_0 |Y]$
Notice that the $Y_i$ here are ancillary so if we pick $\delta_0$ function of CSS, easy. 


If we can make $\delta_0$ a function of CSS, then we can apply Basu
\begin{recipe}[MRE for Square Error Loss]
	Choose $\delta_0$ equivariant function of CSS. Then $\delta^* = \delta_0 - E_0 \delta_0$ is MRE. 
	\begin{example}[$X_i \sim N(\theta,1)$]
	Let $\delta_0 = \bar X$. Then $E_0 \delta_0 = 0$. So $\bar X$ is MRE.
\end{example}
\end{recipe}
Note for absolute error, just do median$_{\theta=0}[\delta(X)|Y]$ instead of expectation. Ie find $m$ such that $P(X\leq m) \geq 1/2$ and $P(X\geq m) \geq 1/2$. 

\begin{theorem}[Existence of MRE]
If loss is convex and not monotone, then MRE exists by the previous theorem. If strictly convex, unique. 
\end{theorem}





\begin{fact}[MRE is Unbiased (under squared error loss)]
Since the bias does not depend on $\theta$, just subtract off whatever bias. 
\end{fact}

\begin{fact}[UMVU is MRE if UMVU is location equivariant]
In a location model, the UMVU is location equivariant. Since MRE is the best amongst equivariant estimators, and any competing equivariant estimators are unbiased..
\myworries{Is this just with square error loss?}

\end{fact}


\begin{theorem}[Anderson's Lemma]
\myworries{Review this}
$Z\sim N(0,\Sigma)$, and $\ell$ is bowl shaped, then $\E \ell (Z) \leq \E \ell (Z+U)$ for any $U\indep Z$. 
\end{theorem}

 \begin{definition}[Pitman Estimator]
\end{definition}
\begin{theorem}[Mini-convolution theorem]
\end{theorem}

\section{James-Stein Estimator}
Setup: $\calN(\mu,\sigma^2)$ with $\sigma^2$ known. 

\begin{theorem}[SURE - Stein Unbiased Risk Estimator ]
Letting $\hat \mu(x) = x+g(x)$ for $g: \R^p \to \R^p$ almost differentiable, and assume that $\E[\sum_{i=1}^p |\partial_i g_i (X)|] <\infty$. Then:

$$\E_\mu[\|\hat \mu(X) - \mu\|^2] = p\sigma^2 + \E\left[\|g(X)\|^2 + 2\sigma^2 \sum_{i=1}^p \partial_i g_i(X)\right].$$
Proved using integration by Parts -- see Lec 17 300c.
\end{theorem}
\begin{fact}[UMVU is not admissible in $\calN(\mu,1)$ model]
Because James Stein renders $X$ inadmissible. 	
\end{fact}

J-S estimator is given by:

$$\hat \mu^{JS} (X) = \left ( 1-\frac{\sigma^2(p-2)}{\|X\|_2^2}\right)X,$$
biased towards the origin. Prove that it has better risk by SURE estimator. 

\section{Bayes Estimators}
Average risk over some choice of prior $\Lambda(\theta)$ on parameter space. 

Want to minimize:

$$\int R(\theta, \delta) d\Lambda(\theta) = \E_{(X,\Theta)} L(\Theta, \delta(X)).$$

So our $P_\theta$ is $X|\Theta = \theta$. By tower property, we can minimize this by minimizing the following for almost all $x$:

$$\arg \min_{\delta} \E [L(\Theta, \delta(X) \mid X= x]$$
ie this is an integral with respect to posterior distribution. For square error: $\delta = \E[g(\Theta)\mid X]$, abs error gives $\delta = \operatorname{median} g(\Theta) |X$. See the quals notes for a list. For square error, often (?) gives a convex combination of UMVU and the prior mean. 
\begin{theorem}[When Bayes is Unique?]
???
\end{theorem}
\begin{fact}[Unique Bayes is Admissible]
Idea is that if $R(\hat \theta ' ,\theta) \leq R(\hat \theta , \theta)$ for all $\theta$, it would then be Bayes. \\ provided the prior isn't super weird (eg continuous dist with an atom) 

\end{fact}

\begin{fact}[Constant risk Bayes is minimax]
If not, some other estimator would render Bayes inadmissible, which would make that estimator the Bayes estimator.
\end{fact}

\begin{fact}[Bayes is not UMVU if $r_\Lambda <\infty$]
Under square error loss, Bayes estimators are biased, unless $r_\Lambda = 0$.
\end{fact}
An example of an unbiased Bayes estimator would be if $X\sim Bin(n,\theta)$ and $\Lambda$ puts mass on $\{0,1\}$ only. Then there exists an estimator of $0$ risk-- $X/n$. 

\section{Minimax}
Want $\delta$ to minimize $\sup_{\theta \in \Omega} R(\theta, \delta)$. \\\\

Note that for any prior, the bayes risk is upper bounded by the minimax (frequentist) risk. 

If $\delta'$ is our guess:

$$\inf_\delta \int R(\delta, \theta) d\Pi (\theta) \leq \inf_\delta \sup_\theta R(\delta, \theta) \leq \sup_\theta R(\delta ', \theta).$$

If we can find a prior such that $\delta'$ is Bayes, then 
$\inf_\delta \int R(\delta, \theta) d\Pi (\theta) = \int R(\delta' , \theta) d\Pi(\theta)$. If this also equals the sup risk RHS, then $\delta'$ is minimax. \\ \\ 

Next best thing is a sequence $\Pi_n$ such that $r_{\Lambda_n} \to \sup_\theta R(\delta ', \theta)$. Then we squeeze the above inequality by taking a limit in $n$-- so that 

$$\inf_\delta \sup_\theta R(\delta, \theta) = \sup_\theta R(\delta', \theta)$$. 



\begin{recipe}[Minimax - Constant risk]
	A Bayes or admissible estimator is constant risk, then it is minimax. \\\\
	
	So: \textbf{find a prior with constant frequentist risk. }
	\begin{enumerate}
		\item Hopefully conjugate prior
		\item Find Bayes estimator 
		\item Write the frequentist risk $R(\theta,\delta_{\Lambda}) = \E_X L(\theta, \delta_{Lambda})$
		\item Find parameters of the prior $\Lambda$ such that $R(\theta,\delta_\Lambda)$ is constant
		\item Conclude this estimator is minimax
	\end{enumerate}
	If unique Bayes, unique minimax. Prior $\Lambda$ is \textit{least favorable}.\\ \\ 
	
	The above is most useful in case when parameter space $\Theta$ is compact. Is the supremum achieved by some $\Lambda$, making the frequenist constant, so that the inequality at beginning of section is equality. 
\end{recipe}

\begin{example}[Multinomial minimax for $p$]
See Theory Coaching. Use conjugate prior Dirichlet. One idea is that a least favorable prior must be symmetric in $\alpha_1,\ldots, \alpha_n$. \\ \\ 

Analogous to in class Binomial minimax for $p$ using Beta prior. 
\end{example}

\begin{definition}[Minimax - Least favorable prior]
$\Lambda$ least favorable if $r_\Lambda \geq r_{\Lambda'}$ for any other $\Lambda'$. \\\\
A sequence of priors $\Lambda_m$ is least favorable if for any $\Lambda'$, $r_{\Lambda'} \leq \lim_m r_{\Lambda_m}$.
\end{definition}

\begin{recipe}[Minimax - Least favorable sequence of priors]
If $\sup R(\theta,\delta) = r$ and have a sequence such that $r_{\Lambda_m} \to r$ then $\Lambda_m$ is least favorable and $\delta$ is minimax. 
\begin{enumerate}
	\item Guess some $\delta$ by looking at limit of Bayes estimator
	\item Find the sup risk of our guess $\delta$, ie $\sup_\theta R(\delta, \theta)$.
	\item Find a sequence of priors s.t. $r_{\Lambda_m} \to r$ 
	\item Conclude $\delta$ minimax
\end{enumerate}
\end{recipe}
\begin{example}[Normal location model minimax]
If $X_i \simiid N(\theta,\sigma^2)$ with $\sigma$ known, then we can show that $\bar X$ is minimax by the above approach. Let $\Lambda_m = N(0,b_m^2)$ with $b_m \to \infty$ eg $b_m = m$. Note that $\sup R(\theta, \bar X_n) = \sigma^2/n$-- it's constant risk so a good candidate. 

$$r_{\Lambda_m} = \E [\Var (\Theta|X)] \to r$$

Conclude $\bar X$ is minimax.
\end{example}

Connection that just because an estimator is minimax doesn't mean it's admissible. $\bar X$ is UMVU, minimax, MRE.. but not admissible. 
\begin{recipe}[Minimax in subfamily argument]
$\delta$ minimax in $\Lambda_0$ then it is also minimax in $\Lambda$ \textbf{if} 
$$\sup_{\theta\in \Lambda_0} R(\theta,\delta) = \sup_{\theta \in \Omega} R(\theta,\delta)$$
Eg, $\bar X$ in $\Lambda_0 = \{\sigma^2 = b\}$ and $\Lambda = \{\sigma^2 \leq b\}$. 
\end{recipe}
Can extend to non parametric big family-- $\bar X$ is minimax if $X_i \simiid F$ since don't increase sup risk in the larger family. 


\newpage
\section{Testing}
Types of errors:
\begin{enumerate}
	\item Type 1: Reject the null when the null is true (restricted to size $\alpha$ size of test)
	\item Type 2: Retain the null when the null is false (given by 1-power)
\end{enumerate}
\subsection{Simple-simple}

\begin{theorem}[Neyman-Pearson (NP) Lemma]
For simple versus simple, a MP test always exists and is given by 

$$\phi(x) = \begin{cases}
	1 \quad p_1(x) >kp_0(x)\\
	\gamma \quad p_1(x) = kp_0(x)\\
	0 \quad p_0(x) <kp_0(x)
\end{cases}$$
where constants determined by level constraint. This is just the likelihood ratio test. First find a $k$, then find the $\gamma$. \\\\ Also note that MP test must have level exactly $\alpha$ unless there exists a test with power $1$ with size $<\alpha$. \\
\end{theorem}
Note that non-uniqueness comes from the $=$ condition-- how to randomize. 
\begin{fact}[NP characterizes MP tests]
The NP is actually an if and only if statement. If MP test, then satisfies the above along with level constraint. MP test must be of this form.
\end{fact}
\subsection{Simple-Composite }
A UMP test only exists if the NP test is the same for \textbf{all} alternatives! If not, have different MP tests, no UMP test! If you want to show that a UMP does not exist, show that the MP test depends on the alternative-- to show exist, does not depend. Eg, testing normal $\theta=0$ versus $\theta\neq 0$, we get different MP tests depending on which side our alternative is on. 




 Occurs most often in a one-dimensional parametric family with monotone-likelihood ratio. 
\begin{definition}[Monotone Likelihood Ratio (MLR)]
If exists $T: \cal X \to \R$ statistic that likelihood ratio $p_{\theta'}/p_{\theta_0}$ is \textbf{non-decreasing} in $T$ \textit{whenever $\theta_0$ < $\theta'$. } \\\\
Ie if we have a singleton null and for alternative $\calH_1: \theta'> \theta_0$, then we check whether there's a statistic such that likelihood ratio is increasing.  
\end{definition}


\begin{theorem}[UMP in MLR]
Testing $\theta = \theta_0$ versus $\theta>\theta_0$, if there's a MLR, then the UMP test given by

$$\phi(x) = \Ind[T(X) > C ] + \gamma \Ind[T(X) = C].$$
\end{theorem}
\begin{fact}[MLR in Exponential Families]
Exponential families satisfy MLR! Also satisfied in non-central chi-square.
\end{fact}



\subsection{Composite-Simple}



An important insight is that:

$$\{ \text{tests valid for full composite null} \} \subset \{ \text{tests valid for a specific point in the null}\}.$$
So if you can find a best test for a specific point, and it is also valid for the full null, then it is the best for the composite null. \\\\

If we have a composite null, to find UMP test we put a prior on the null parameter space and reduce to a simple-simple. If the MP test in this simple-simple problem is also \textbf{valid} for the original problem null, then it is the UMP for composite-simple testing problem. Ie:

$$\tilde P_0 (\cdot) = \int_{H_0} P(\cdot) d\Pi(P) \quad \text{weighted average of null measures}$$
\begin{definition}[Least favorable prior]
The prior with the lowest powered MP test, ie $\beta_{\Lambda} \leq \beta_{\Lambda'}$ for all $\Lambda'$ prior on $\Theta_0$. 
\end{definition}

\begin{recipe}[Composite-Simple]

To find UMP test:
\begin{enumerate}
	\item Posit a least favorable prior
	\item Compute the simple-simple MP test $\phi_\Pi$ wrt the simple null from the LFP
	\item Show that $\phi_\Pi$ is \textit{valid} for the original problem, ie $\E_{\theta_0} \phi_\Pi \leq \alpha$ for all $\theta_0 \in \calH_0$.
	\item Conclude that $\phi_\Pi$ is UMP in the original problem
\end{enumerate}
Unique UMP if $\phi_\Pi$ is unique in the simple-simple case.\\\\
For least favorable prior, try to make it hard to reject. Ie, mass on the boundary. 
\end{recipe}
\subsection{Composite-Composite}
UMP rarely exists in composite-composite. To check, always first fix an alternative, find MP test for composite-simple case. Then show that it does or doesn't depend on the alternative chosen. Here are the times when it does:

\begin{enumerate}
	\item One-side MLR testing: $\calH_0: \theta\leq \theta_0$, $\calH_1: \theta>\theta_0$
	\item Bioequivalence: Test null that $\theta \notin (\theta_1,\theta_2)$. If we have a 1-param expo family, then $\phi(x) = \Ind[T\in (C_1, C_2)]$ is UMP
	\item One sided testing of Gaussian mean, ie $\calH_0: v^T \mu = \delta$ versus $\calH_1: v^T \mu >\delta$ for fixed $v$ with $v^T \Sigma v>0$, then reject for large $v^T X$. 
\end{enumerate}


\subsection{Examples}

\begin{example}[$X\sim N(\mu_x, \sigma^2$, $Y\sim N(\mu_y,\sigma^2)$ with $\sigma$ known]
Want to test $\mu_y\leq \mu_x$ versus not. 
\begin{enumerate}
	\item Fix an alternative $\theta_x, \theta_y$. 
	\item Put a point mass prior on average of the $\mu_x = \mu_y$ line ie $(\theta_x + \theta_y)/2$ for both
	\item Get NP test: $\frac{Y-x}{\sqrt{2} \sigma} > z_{1-\alpha}$
	\item Need to check that level $\alpha for original problem$. So fix a null $\mu_x,\mu_y$ such that $\mu_y\leq \mu_x$. 
	\item Conclude that MP for testing $\calH_0$ versus $(\theta_x, \theta_y)$
	\item Since doesn't depend on the specific alternative chosen, it's UMP
\end{enumerate}
\end{example}
\begin{example}[Non-parametric - hypothesis about the cdf $\calH_0: F_{X_i} (u) \geq p_0$]
Idea is to parametrize $P$ by conditional distributions:
\begin{enumerate}
	\item $p = P(X_i \leq u)$
	\item $P^-$ Distribution of $X_i| X_i \leq u$
	\item $P^+$ Distribution of $X_i| X_i > u$
\end{enumerate}
Least favorable prior will be have the same conditional distributions with a different $p$ such that in the null space. (Point mass). Becomes isomorphic to testing $Bin(n,p)$ $p \geq p_0$. 
\end{example}

Weird: we have a MP but not UMP for testing $H_0: \sigma \geq \sigma_0$ where both $\mu,\sigma$ unknown-- place point mass LFP. By contrast, testing $\sigma \leq \sigma_0$ we can't use a point mass- need a normal prior. Leads to UMP test. $\phi = \Ind[\frac{\sum_{i=1}^n (X_i - \bar X)^2 }{\sigma_0^2} > c_{n-1, 1-\alpha}]$. 

\subsection{P-values}

\begin{definition}[p-value]
Given some nested rejection regions for valid test, then
$$\hat p(X)= \inf \{\alpha: X\in R_\alpha\}.$$
\end{definition}
\begin{fact}[P-values are subuniform]
P values are subuniform. If we have exact level $\alpha$ for any $\alpha$, then $\hat p$ is exactly uniform.
\end{fact}
%% end 300a
%% 300b 
\newpage
\section{Stochastic Convergence}
Helpful tools
\begin{enumerate}
	\item Don't forget Portmanteau
	\item CLT, SLLN 
	\item CMT
	\item Slutsky
\end{enumerate} 
\begin{theorem}[CMT]
If $g$ is continuous on a set of probability $1$
$$X_n\to^* X \implies g(X_n) \to^* g(X)$$
for conv in dist, prob, or as
\end{theorem}
\begin{theorem}[Slutsky]
A few parts
\begin{enumerate}
	\item $X_n \condist c \implies X_n \conprob c$
	\item If $\|X_n - Y_n\| \conprob 0$ then if $X_n \condist X$ we have also $Y_n \condist X$. 
	\item If $X_n \condist X$ and $Y_n \conprob c$ then
	$$\begin{pmatrix}
		X_n\\Y_n 
	\end{pmatrix}\condist \begin{pmatrix}
		X\\c 
	\end{pmatrix}$$
	\end{enumerate}
	Corrolary via Slutsky and CMT:
	$X_n\condist X$ and $Y_n \conprob c$ gives us 
	\begin{enumerate}
	\item $Y_n X_n \condist cX$
	\item $Y_n + X_n \condist c+ X$
	\item $Y_n^{-1} X_n \condist c^{-1} X$
	\end{enumerate}

\end{theorem}
\begin{definition}[Uniform Tightness]
A collection $\{X_\alpha\}_{\alpha \in A}$ is uniformly tight if for all $\epsilon>0$ there exists an $M<\infty$ such that 

$$P(\|X_\alpha\|>M) \leq \epsilon \quad \text{for all } \alpha \in A$$
\end{definition}
\begin{example}[Markov, Tightness]
If all $X_\alpha$ have the same $\ell$-th moment, then just use Markov to prove tightness. 
\end{example}
In general if a collection has increasing means or something, it won't be tight, eg $X_n \sim N(n,1)$. 

\begin{theorem}[Prohorov]
Uniformly tight $\implies$ for all sequences there is a a subsequence that converges in distribution to a random variable.\\\\
Conversely, 
Convergence in distribution $\implies$ unifromly tight. 
\end{theorem}


\subsection{Big-O, Little-o}
Non-stochastic versions, 
$$f(x) = O(g(x)) \iff \limsup_{\eps\to 0} \frac{f(\epsilon)}{g(\eps)} < \infty$$.
$$f(x) = o(g(x)) \iff \lim_{\eps \to 0} \frac{f(\eps)}{g(\eps)} = 0 $$

\begin{definition}[Little-${o_p}$]
$$X_n = o_p(R_n) \iff \exists Y_n \text{ such that} X_n = R_n Y_n \text{ with } Y_n \conprob 0$$
\end{definition}
\begin{definition}[Big $O_p$]
$$X_n = O_p(R_n) \iff X_n = R_n Y_n \text{ where } Y_n \text{ uniformly tight}$$
\end{definition}

\begin{theorem}[Delta Method]
Let $r_n\to \infty$ and $f$ differentiable at $\theta$. If $r_n (T_n -\theta)\condist A$ then:
\begin{enumerate}
	\item $$r_n(f(T_n) - f(\theta)) \condist f'_\theta A$$
	\item $$r_n(f(t_n) - f(\theta)) - f_\theta'(r_n (T_n - \theta)) \conprob 0$$
\end{enumerate}
\begin{proof}
	Main idea:
	$$f(\theta + h) - f(\theta) = f'_\theta h + o(\|h \|) \text{ as } h\to 0$$
	Take $h = T_n - \theta$ 
\end{proof}

\end{theorem}
\begin{theorem}[Higher order delta method]
\end{theorem}

\section{MLE}
\begin{definition}[M-estimators]
$$\hat \theta_n = \arg \max_\theta \frac{1}{n} \sum_{i=1}^n m_\theta(X_i)$$
where $m_\theta$ is some known function, eg $\ell_\theta$-- maximize log likelihood 

\end{definition}
\begin{definition}[Z-estimator]
$$\hat \theta _n = \{\theta: n^{-1} \sum_{i=1}^n \Psi_\theta(X_i) = 0\}$$
eg $\grad \ell_\theta = 0$. 
\end{definition}
\subsection{Consistency of MLE}
Consistency of $P_n \ell_\theta \conprob P \ell_\theta $ (WLLN) is \textbf{not} enough for consistency of MLE $\hat \theta_n \conprob \theta^*$. For arbitrary sample size, $P_n \ell_\theta$ not necessarily maximized at (or near) $\theta^*$ if we converge too non-uniformly. See picture.  For consistency of MLE, need:
\begin{enumerate}
	\item Uniform convergence (in probability)
	\item Well-separation 
\end{enumerate}
\begin{definition}[Uniform convergence]
$M_n(\theta)$ converges uniformly to $M(\theta)$ if 
$$\sup_{\theta} |M_n(\theta - M(\theta)| \conprob 0$$
\end{definition}
\begin{definition}[Well-separation]
\myworries{definition........}\\
eg, strong convexity. 
\end{definition}
Some more primitive conditions. Identifiability and a finite sample space $|\Theta|<\infty$ are enough for consistency of MLE. 
\begin{definition}[Identifiability]
Identifiable if for $\theta \neq \theta'$, $P_\theta \neq P_\theta'$ ie KL divergence is strictly positive. 
\end{definition}
\begin{example}[Example of non-identifiability]
In my notes
\end{example}

\subsection{Asymptotic normality of the MLE}
Under a regularity condition ("smooth, nice"), the MLE is normal.
\begin{definition}[Smooth/Nice at $\theta$]
See notes.. \begin{enumerate}
	\item Hessian of log likelihood is Lipschitz near $\theta^*$ - see notes 
	\item Bounded gradient $P_\theta^* \| \grad \ell_\theta^* \|^2 <\infty $
\end{enumerate}
\end{definition}
\begin{theorem}[Asymptotic normality of MLE]
If \begin{enumerate}
	\item $\{P_\theta\}_{\theta \in \Theta}$ is smooth/nice at $\theta^*$
	\item $\Theta$ open subset of $\R^d$,  
	\item Hessian has finite mean (or alt that exchange order of differentiation wrt $\theta$ and expectation).
	\item $\hat \theta_n$ the MLE is  consistent 
\end{enumerate}
Then $\sqrt{n} (\hat \theta_n - \theta) \condist N(0,\Sigma_\theta^*)$.
\textbf{If we can also exchange order of differentiation and expectation} , $\Sigma_\theta^* = I_\theta^{-1}$. 
\end{theorem}
\section{Fisher Information}

\begin{definition}[Fisher Information]
Outer product of the score
$$I_\theta = \E_{P_\theta} [\grad_\theta \ell_\theta (\grad_\theta \ell_\theta)^T]$$
If we can exchange order of differentiation and expectation then:
$$I_\theta = \Cov (\grad \ell_\theta) = -\E[\grad^2 \ell_\theta] $$
\end{definition}
Lots of information, small variance. Look at hessian to look at curvature-- if it's really peaked, we have lots of information. 
\begin{theorem}[Cramer-Rao]
\end{theorem}
\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Distribution} & \textbf{Parameter(s)} & \textbf{Fisher information $I(\theta)$} \\
\hline
Bernoulli $\operatorname{Bern}(p)$                         & $p\in(0,1)$              & $\displaystyle \frac{1}{p(1-p)}$ \\[4pt]
Binomial $\operatorname{Bin}(m,p)$ \,(fixed $m$)           & $p\in(0,1)$              & $\displaystyle \frac{m}{p(1-p)}$ \\[4pt]
Poisson $\operatorname{Pois}(\lambda)$                     & $\lambda>0$              & $\displaystyle \frac{1}{\lambda}$ \\[6pt]
Exponential $\operatorname{Exp}(\lambda)$                  & $\lambda>0$              & $\displaystyle \frac{1}{\lambda^{2}}$ \\[6pt]
Gamma $\operatorname{Gamma}(\alpha,\theta)$ \,(fixed $\alpha$) & $\theta>0$          & $\displaystyle \frac{\alpha}{\theta^{2}}$ \\[6pt]
Normal $\mathcal N(\mu,\sigma^{2})$ \,(known $\sigma^{2}$) & $\mu\in\mathbb R$        & $\displaystyle \frac{1}{\sigma^{2}}$ \\[8pt]
Normal $\mathcal N(\mu,\sigma^{2})$ \,(known $\mu$)        & $\sigma^{2}>0$           & $\displaystyle \frac{1}{2\sigma^{4}}$ \\[10pt]
\multicolumn{2}{|l|}{Normal $\mathcal N(\mu,\sigma^{2})$ \,(both unknown)}
  & $\displaystyle
    \begin{pmatrix}
      \frac{1}{\sigma^{2}} & 0 \\[6pt]
      0 & \frac{1}{2\sigma^{4}}
    \end{pmatrix}$ \\[10pt]
\hline
\end{tabular}
\caption{Per‐observation Fisher information for common parametric families.  
Multiply by $n$ for a sample of size $n$.}
\end{table}


\begin{definition}[Asymptotic efficiency]
\end{definition}
\end{document}