\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{hw}
\usepackage{tcolorbox}
\usepackage[ruled,vlined]{algorithm2e}

% ------------------  Bibliography  ------------------
%\usepackage[
%  backend=biber,
%  style=numeric,      % plain numbers: [1]
%  sorting=none,       % keep refs in citation order
%  maxbibnames=99      % show all authors in bib
%]{biblatex}
\usepackage{booktabs}
% ----------------------------------------------------

\tcbuselibrary{breakable}
\title{300a Quals Guide}
\newcommand{\answer}[1]{%
  \begin{tcolorbox}[
    colback=gray!9.8,
    boxrule=0.5pt,
    breakable]
  \small #1
  \end{tcolorbox}}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\newcommand{\simiid}{\overset{iid}\sim }
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\row}{\operatorname{row}}
\newcommand{\Proj}{\operatorname{Proj}}
\newcommand{\Unif}{\operatorname{Unif}}

\begin{document}
\maketitle
\section{Misc facts}
\begin{definition}[Convexity]
For all $0\leq t\leq 1$ and $x_1,x_2 \in X$:
$$f(tx_1 + (1-t)x_2) \leq tf(x_1) + (1-t) f(x_2).$$
Alternative, check the second derivative $\geq 0$
\end{definition}
\begin{theorem}[Jensen's Inequality]
If $\phi$ convex, then $\E \phi(X) \geq \phi(EX)$. Eg $EX^2 \geq (EX)^2$. Inequality is strict if $\phi$ is strictly convex and $X$ is not degenerate (constant). Also conditional version holds. 
\end{theorem}
\section{Exponential Families}
\begin{definition}[Exponential Family]
	$$p_\theta(x) = \exp(\sum_{i=1}^s T_i(X) \eta_i(\theta) - A(\theta))h(x)$$
	$$p_\eta(x) = \exp(\sum_{i=1}^s T_i(X) \eta_i - \tilde A(\eta)) h(x)$$
\end{definition}
\begin{fact}[Expectation and covariance of sufficient stats in exponential fam]
$$\E[T(X)] = \grad A(\eta)$$
$$\Cov (T_i(X), T_j(X)) = \partial_{\eta_i} \partial_{\eta_j} A(\eta)$$
\end{fact}

\subsection{All the Expo family examples}

Include curved.. 


\section{Sufficiency} 
"Throwing away everything else besides this statistic entails no loss of info in estimating $\theta$"
\begin{definition}[Sufficiency]
If for all $t$, the distribution of $X|T=t$ does not depend on $\theta$.
\end{definition}
If you collect data $X_1,\ldots,X_n$, then throw away data except for $T=t$, then can construct $\tilde X_1,\ldots \tilde X_n$ with same distribution as $\underline{X}$ just by knowing $T$.

\myworries{Make a table of good examples including $\Unif(0,\theta)$}
\begin{theorem}[Factorization Theorem]
$T$ sufficient for $\theta$ (or for the model) $\iff$ we can write $p_\theta(\underline{x}) = g_\theta(t(\underline{x})) h(\underline{x})$. Ie, can factor the density into a part that depends on statistic and $\theta$, and another part that depends on data but not $\theta$. 
\begin{example}[Expo($\theta$)]
Can write $p_\theta(\underline x) = \theta^n \exp(-\theta\sum x_i)$ so $T(X) = \sum_{i=1}^n X_i$ is sufficient.
\end{example}
\begin{example}[Exponential families]
Look at the form -- $T$ is sufficient.
\end{example}


\end{theorem}
\begin{definition}[Minimal sufficiency]
For all $T'$ sufficient, $T$ is a function of $T'$.
\end{definition}


\begin{theorem}[Rao-Blackwell]
If $T$ sufficient, $L$ is any convex loss, and $\delta$ estimates $g(\theta)$, define:

$$\eta(T) = \E[\delta(X)|T],$$
then using Tower property and Jensen's:

$$R(\theta, \delta(X) = \E[g(\theta), \delta(X)] \geq \E [L(\theta, \eta(T)) = R(\theta, \eta(T))$$
\end{theorem}
\begin{definition}[Ancillary Statistic]
Distribution of $A(X)$ does not depend on $\theta$.
\begin{definition}[First Order Ancillary]
If $\E_\theta A(X)$ does not depend on $\theta$. Weaker than Ancillary. 
\end{definition}
\end{definition}
\section{Completeness} 
\begin{definition}[Completeness]
$T(X)$ complete if no non-constant function of $T$ is even 1st order ancillary, or equivalently:

$$\E f(T) = 0 \implies f(T) = 0 \text{ a.s}.$$
\end{definition}
\begin{example}[$X_i\sim\Unif(0,\theta)$]
See coaching notes-- break $h$ into positive and negative parts to show $h = 0$ as. 	
\end{example}
\begin{theorem}[Full rank exponential family]
Ie if $(\eta_1,\ldots, \eta_k): \eta \in \Omega$ contains a $k-$dimensional rectangle, then $T$ is not only sufficient, but \textbf{also complete}.
\end{theorem}
\begin{theorem}[Basu's Theorem]
If $T$ is CSS and $A$ Ancillary, then $T\indep A$. 
\end{theorem}
A common strategy with Basu's theorem is to consider a submodel-- show independence in the submodel, then based on arbitrariness of submodel, show true for full model. 
\begin{example}[Show sample mean and sample variance in $N(\mu,\sigma^2)$ are independent]
(Assume both unknown). then in the submodel where $\sigma = \sigma_0$ known, then $\bar X$ is CSS. Can show that $\sum (X_i - \bar X)^2$ is Ancillary. So $\bar X \indep \sum (X_i - \bar X)^2$. This is true for all $\mu$, and for $\sigma = \sigma_0$ fixed. But $\sigma_0$ arbtirary, so true for all $\mu,\sigma$. 	
\end{example}
\myworries{Add a table here including 2 param Unif}
\begin{enumerate}
	\item $\Unif(0,\theta)$ : $T= X_{(n)}$
	\item $\Unif(\theta_1, \theta_2)$: $T = (X_{(1)}, X_{(n)})$
\end{enumerate}




\subsection{Epic CSS rolodex}


\section{UMVU}
Note that we can't take existence of an unbiased estimator for granted. If $x\sim Bin(n,\theta)$. Take like $g(\theta) = \frac{1-\theta}{\theta}$ or a polynomial of degree $>n$-- no unbiased estimators. Strategy to show no unbiased estimator is just write out the expectation of an estimator.
\begin{definition}[UMVU]
$\delta^*$ is unbiased and for all other $\delta$, $R(\delta^*,g(\theta)) \leq R(\delta,g(\theta))$ for all $\theta$.
\end{definition}
Note that if we have an unbiased estimator, we can Rao-Blackwellize with a sufficient statistic and still have unbiased estimator. 
\begin{theorem}[Lehman-Scheffe]
If unbiased estimator is function of CSS, it is UMVU. (Since there exists at most one unbiased estimator that's function of $T$ by completeness).\\\\
Alternate statement: if there exists any unbiased estimator and a CSS $T$, then there is a unique unbiased estimator that's a function of $T$, which is UMVU (UMRU for any convex loss). Unique UMRU if strict convex loss, since this makes Jensen strict. 

\end{theorem}
\begin{recipe}[UMVU with CSS]Steps:
	\begin{enumerate}
	\item Find a CSS
	\item Find an unbiased estimator function of CSS. (Alternatively, find a dumb unbiased estimator and RB)
	\item $\implies$ UMVU
\end{enumerate}
\end{recipe}
Note that UMVU can be inadmissible - see James-Stein or the Poisson UMVU for $g(\theta).= \exp(-3\lambda)$ example in Lecture 5. 
\begin{theorem}[Orthogonality Condition]
Let $\hat \theta$ be an unbiased estimator with $E_P \hat\theta^2 <\infty$ if $U(X)$ is an unbiased estimator of $0$ for all $P$, then:

$$\hat \theta \text { UMVU } \iff E_P [\hat \theta(X) U(X) ] = 0 \text { for all unbiased estimators of } 0 \text{ and for all P}$$
An application is showing that the addition of UMVUs is UMVU. Also, product of UMVUs is UMVU for its expectation. \\
Also reasonable to try to show not UMVU. 
\myworries{Come back to Ex 2.3 in Notes}
\end{theorem}
\begin{theorem}[Cramer Rao LB]
For any $\hat \theta$ unbiased for $\theta$:

$$\Var_\theta \hat \theta \succ I_\theta^{-1}$$
when $\{P_\theta\}$ is QMD with non singular $I_\theta$. 
\end{theorem}

\subsection{UMVU Examples}
\begin{enumerate}
	\item Basic Expo Fam examples -- eg $\bar X$ in Bernouli or Normal with known variance 
	\item Empirical CDF in $\mathcal N(\theta,1)$.
	\begin{itemize}
		\item CSS is $\bar X$, $\delta = \Ind[X_1 < u]$ unbiased. 
		\item Idea to RB then add and subtract $\bar X$ since $X_1 - \bar X$ is ancillary, then apply Basu
		\item UMVU is $\Phi(\frac{u - \bar X}{\sqrt{(n-1)/n}})$
	\end{itemize}

\end{enumerate}
Non parametric examples
\begin{enumerate}
	\item $X_i \sim F \in \calF = \{ \text{ all distributions with density wrt Lebesgue and finite variance } \}$. $g(\theta) = E_F X_i$.
	\begin{itemize}
		\item Note that $\bar X$ is unbiased in the big family and is UMVU in the normal \textbf{subfamily} 
		\item Order statistics CSS (always sufficient - complete by subfamily arg and bijection with sums of powers): $(X_{(1)}, \ldots, X_{(n)}) \iff (\sum X_i, \ldots, \sum X_i^n)$ bijection. 
		\item So $\bar X$ is UMVU
	\end{itemize}
	\item $X_i$ iid symmetric about $\theta$, $E X_i = \theta$. Finite variance. 
	\begin{itemize}
		\item Two subclasses: normal family, $\Unif(\theta_1, \theta_2)$ family - have different UMVUs and both are unbiased in the original class
	\end{itemize}
\end{enumerate}
\begin{recipe}[Subfamily UMVU Argument]
If UMVU in subfamily and unbiased in big family, must be UMVU in big family \textbf{if} an  UMVU exists in the big family. Because the UMVU \textbf{uniquely} minimizes variance in the subfamily. 
\begin{proof}
	Take the big family. Take a function such that $E_\theta f(X) = 0$ for all $\theta$. Then true for subfamily. So $P_\theta (f(X) = 0)$ for all $\theta$ in the subfamily. Same null sets as big family, means that also $P_\theta (f(X) = 0)$ for the big family.
\end{proof}
\end{recipe}
\begin{fact}[Completeness in subfamily]
If $\calF_0 \subset \calF$ and they have the same null sets, then completeness in $\calF_0$ implies completeness in $\calF$. 
\end{fact}
\begin{recipe}[Non-existence of Non-parametric UMVU]
Find two different subclasses with different unique UMVU that are also unbiased in the big class -- no UMVU in big class.	
\end{recipe}
If we can't use Lehman-Scheffe, try one of the following ideas:
\begin{enumerate}
	\item Cramer-Rao lower bound
	\item Orthogonality condition
	\item Subfamily arguments
\end{enumerate}
\subsection{Non-convex loss functions}
If loss is bounded, there is no UMRU estimator. This is unbiased:
$$\delta_\pi = \begin{cases}
	g(\theta_0) \text{ wp } 1-\pi\\
	\frac 1 \pi [\delta_0 (X) - g(\theta_0)] + g(\theta_0)
\end{cases}$$
and its risk is $\pi M$, so it could be arbitrarily small.

\subsection{UMVU Tools}
\begin{enumerate}
	\item Find a CSS and do Lehman Scheffe
	\begin{itemize}
		\item Directly show CSS
		\item Use exponential family
	\end{itemize}
	\item Orthogonality condition
	\item Cramer Rao
	\item Subfamily
\end{enumerate}


\section{MRE}
Big picture-- there's an easy recipe for MRE for square error or absolute error if we pick any old $\delta_0$ equivariant function of CSS. 

\begin{definition}[Location models]
Density satisfies
$$f_{\theta + h} (x+h) = f_\theta(x)$$
Think: normal at its mean has same density as a shifted normal at shifted mean.\\ 
A \textbf{location invariant loss} is $\ell(a + h,\theta + h ) = \ell(a,\theta)$ for all $a, \theta, h$. 
\end{definition}
Want location equivariant estimators, ie $\hat \theta(x + h) = \hat(\theta) + h$ -- if our observations all shift by some amount, our estimate of the location should also shift by that amount. 

\begin{fact}[Bias, variance, risk of equivariant estimators]
Do not depend on $\theta$. Ie, 
$$\E_\theta \hat \theta = \theta + b \text{ for all } \theta $$
for risk, since it's constant, we can hope to find the best among all equivariant estimators.
\end{fact}

\begin{definition}[MRE]
Satisfies equivariance condition: $$\hat \theta(x+c, u) = \hat \theta (x,u) + c$$
and minimum risk condition amongst all equivariant estimators. 
\end{definition}
\begin{definition}[Characterization of location \textbf{invariant} estimators]
Location invariant means $U(X_1 + c,\ldots,X_n + c) = U(X)$. Characterize by:
$U$ location invariant $\iff U = V(y_1, \ldots, y_{n-1})$ where $y_i = X_i - X_n$. 
\end{definition}
\begin{fact}[Characterization of location \textbf{equivariant} estimators]
Let $\delta_0$ be \textbf{any} location equivariant estimator, eg $\delta_0 = \bar X$. 
$$\delta \text { is location equivariant} \iff \delta(X_1,\ldots,X_n) = \delta_0(X_1,\ldots,X_n) + U(X_1,\ldots, X_n) $$ 
where $U$ is location \textbf{invariant}. Then from invariant characterization:
$$\iff \delta(X_1,\ldots,X_n) = \delta_0(X_1,\ldots,X_n) + V(Y_1,\ldots, Y_{n-1})$$
\end{fact}

\begin{recipe}[Finding the MRE]
Let $X_i$ observations iid from location model and let $Y = (X_1 - X_n, \ldots, X_{n-1} - X_n)$. Let $\hat \theta_0$ be any location equivariant estimator of $\theta_0$ with finite risk. If the following is well-defined:
$$v(y) = \arg \min _v E_0 [\ell(\hat \theta_0 (X) - v, 0) \mid Y = y]$$
Then there exists an MRE $\hat \theta^*(X) = \hat \theta_0(X) - v(Y)$.	
\end{recipe}

Want to minimize 
	$$\E_\theta [ \rho (\delta_0(X)- V(Y) - \theta)] = \E_0 \rho (\delta_0(X) - V(Y))$$
	Apply Tower property and minimize the inner expectation. For square error yields:
	$$V = \E_0 [\delta_0 (X) |Y)]$$
	So $\delta^* = \delta_0 - E_0 [\delta_0 |Y]$
Notice that the $Y_i$ here are ancillary so if we pick $\delta_0$ function of CSS, easy. 


If we can make $\delta_0$ a function of CSS, then we can apply Basu
\begin{recipe}[MRE for Square Error Loss]
	Choose $\delta_0$ equivariant function of CSS. Then $\delta^* = \delta_0 - E_0 \delta_0$ is MRE. 
	\begin{example}[$X_i \sim N(\theta,1)$]
	Let $\delta_0 = \bar X$. Then $E_0 \delta_0 = 0$. So $\bar X$ is MRE.
\end{example}
\end{recipe}
Note for absolute error, just do median$_{\theta=0}[\delta(X)|Y]$ instead of expectation. Ie find $m$ such that $P(X\leq m) \geq 1/2$ and $P(X\geq m) \geq 1/2$. 

\begin{theorem}[Existence of MRE]
If loss is convex and not monotone, then MRE exists by the previous theorem. If strictly convex, unique. 
\end{theorem}





\begin{fact}[MRE is Unbiased (under squared error loss)]
Since the bias does not depend on $\theta$, just subtract off whatever bias. 
\end{fact}

\begin{fact}[UMVU is MRE if UMVU is location equivariant]
In a location model, the UMVU is location equivariant. Since MRE is the best amongst equivariant estimators, and any competing equivariant estimators are unbiased..
\myworries{Is this just with square error loss?}

\end{fact}


\begin{theorem}[Anderson's Lemma]
\myworries{Review this}
$Z\sim N(0,\Sigma)$, and $\ell$ is bowl shaped, then $\E \ell (Z) \leq \E \ell (Z+U)$ for any $U\indep Z$. 
\end{theorem}

 \begin{definition}[Pitman Estimator]
 Under square error loss, we can derive MRE because it's the Pitman estimator. 
 ...... 
\end{definition}
\begin{theorem}[Mini-convolution theorem]
..... 
\end{theorem}

\section{James-Stein Estimator}
Setup: $\calN(\mu,\sigma^2)$ with $\sigma^2$ known. 

\begin{theorem}[SURE - Stein Unbiased Risk Estimator ]
Letting $\hat \mu(x) = x+g(x)$ for $g: \R^p \to \R^p$ almost differentiable, and assume that $\E[\sum_{i=1}^p |\partial_i g_i (X)|] <\infty$. Then:

$$\E_\mu[\|\hat \mu(X) - \mu\|^2] = p\sigma^2 + \E\left[\|g(X)\|^2 + 2\sigma^2 \sum_{i=1}^p \partial_i g_i(X)\right].$$
Proved using integration by Parts -- see Lec 17 300c.
\end{theorem}
\begin{fact}[UMVU is not admissible in $\calN(\mu,1)$ model]
Because James Stein renders $X$ inadmissible. 	
\end{fact}

J-S estimator is given by:

$$\hat \mu^{JS} (X) = \left ( 1-\frac{\sigma^2(p-2)}{\|X\|_2^2}\right)X,$$
biased towards the origin. Prove that it has better risk by SURE estimator. 

\section{Bayes Estimators}
Average risk over some choice of prior $\Lambda(\theta)$ on parameter space. 

Want to minimize:

$$\int R(\theta, \delta) d\Lambda(\theta) = \E_{(X,\Theta)} L(\Theta, \delta(X)).$$

So our $P_\theta$ is $X|\Theta = \theta$. By tower property, we can minimize this by minimizing the following for almost all $x$:

$$\arg \min_{\delta} \E [L(\Theta, \delta(X) \mid X= x]$$
ie this is an integral with respect to posterior distribution. For square error: $\delta = \E[g(\Theta)\mid X]$, abs error gives $\delta = \operatorname{median} g(\Theta) |X$. See the quals notes for a list. For square error, often (?) gives a convex combination of UMVU and the prior mean. 
\begin{theorem}[When Bayes is Unique?]
???
\end{theorem}
\begin{fact}[Unique Bayes is Admissible]
Idea is that if $R(\hat \theta ' ,\theta) \leq R(\hat \theta , \theta)$ for all $\theta$, it would then be Bayes. \\ provided the prior isn't super weird (eg continuous dist with an atom) 

\end{fact}

\begin{fact}[Constant risk Bayes is minimax]
If not, some other estimator would render Bayes inadmissible, which would make that estimator the Bayes estimator.
\end{fact}

\begin{fact}[Bayes is not UMVU if $r_\Lambda <\infty$]
Under square error loss, Bayes estimators are biased, unless $r_\Lambda = 0$.
\end{fact}
An example of an unbiased Bayes estimator would be if $X\sim Bin(n,\theta)$ and $\Lambda$ puts mass on $\{0,1\}$ only. Then there exists an estimator of $0$ risk-- $X/n$. 

\section{Minimax}
Want $\delta$ to minimize $\sup_{\theta \in \Omega} R(\theta, \delta)$. \\\\

Note that for any prior, the bayes risk is upper bounded by the minimax (frequentist) risk. 

If $\delta'$ is our guess:

$$\inf_\delta \int R(\delta, \theta) d\Pi (\theta) \leq \inf_\delta \sup_\theta R(\delta, \theta) \leq \sup_\theta R(\delta ', \theta).$$

If we can find a prior such that $\delta'$ is Bayes, then 
$\inf_\delta \int R(\delta, \theta) d\Pi (\theta) = \int R(\delta' , \theta) d\Pi(\theta)$. If this also equals the sup risk RHS, then $\delta'$ is minimax. \\ \\ 

Next best thing is a sequence $\Pi_n$ such that $r_{\Lambda_n} \to \sup_\theta R(\delta ', \theta)$. Then we squeeze the above inequality by taking a limit in $n$-- so that 

$$\inf_\delta \sup_\theta R(\delta, \theta) = \sup_\theta R(\delta', \theta)$$. 



\begin{recipe}[Minimax - Constant risk]
	A Bayes or admissible estimator is constant risk, then it is minimax. \\\\
	
	So: \textbf{find a prior with constant frequentist risk. }
	\begin{enumerate}
		\item Hopefully conjugate prior
		\item Find Bayes estimator 
		\item Write the frequentist risk $R(\theta,\delta_{\Lambda}) = \E_X L(\theta, \delta_{Lambda})$
		\item Find parameters of the prior $\Lambda$ such that $R(\theta,\delta_\Lambda)$ is constant
		\item Conclude this estimator is minimax
	\end{enumerate}
	If unique Bayes, unique minimax. Prior $\Lambda$ is \textit{least favorable}.\\ \\ 
	
	The above is most useful in case when parameter space $\Theta$ is compact. Is the supremum achieved by some $\Lambda$, making the frequenist constant, so that the inequality at beginning of section is equality. 
\end{recipe}

\begin{example}[Multinomial minimax for $p$]
See Theory Coaching. Use conjugate prior Dirichlet. One idea is that a least favorable prior must be symmetric in $\alpha_1,\ldots, \alpha_n$. \\ \\ 

Analogous to in class Binomial minimax for $p$ using Beta prior. 
\end{example}

\begin{definition}[Minimax - Least favorable prior]
$\Lambda$ least favorable if $r_\Lambda \geq r_{\Lambda'}$ for any other $\Lambda'$. \\\\
A sequence of priors $\Lambda_m$ is least favorable if for any $\Lambda'$, $r_{\Lambda'} \leq \lim_m r_{\Lambda_m}$.
\end{definition}

\begin{recipe}[Minimax - Least favorable sequence of priors]
If $\sup R(\theta,\delta) = r$ and have a sequence such that $r_{\Lambda_m} \to r$ then $\Lambda_m$ is least favorable and $\delta$ is minimax. 
\begin{enumerate}
	\item Guess some $\delta$ by looking at limit of Bayes estimator
	\item Find the sup risk of our guess $\delta$, ie $\sup_\theta R(\delta, \theta)$.
	\item Find a sequence of priors s.t. $r_{\Lambda_m} \to r$ 
	\item Conclude $\delta$ minimax
\end{enumerate}
\end{recipe}
\begin{example}[Normal location model minimax]
If $X_i \simiid N(\theta,\sigma^2)$ with $\sigma$ known, then we can show that $\bar X$ is minimax by the above approach. Let $\Lambda_m = N(0,b_m^2)$ with $b_m \to \infty$ eg $b_m = m$. Note that $\sup R(\theta, \bar X_n) = \sigma^2/n$-- it's constant risk so a good candidate. 

$$r_{\Lambda_m} = \E [\Var (\Theta|X)] \to r$$

Conclude $\bar X$ is minimax.
\end{example}

Connection that just because an estimator is minimax doesn't mean it's admissible. $\bar X$ is UMVU, minimax, MRE.. but not admissible. 
\begin{recipe}[Minimax in subfamily argument]
$\delta$ minimax in $\Lambda_0$ then it is also minimax in $\Lambda$ \textbf{if} 
$$\sup_{\theta\in \Lambda_0} R(\theta,\delta) = \sup_{\theta \in \Omega} R(\theta,\delta)$$
Eg, $\bar X$ in $\Lambda_0 = \{\sigma^2 = b\}$ and $\Lambda = \{\sigma^2 \leq b\}$. 
\end{recipe}
Can extend to non parametric big family-- $\bar X$ is minimax if $X_i \simiid F$ since don't increase sup risk in the larger family. 

\begin{theorem}[Connection MRE and Minimax]
Pg 340: If $\delta$ is the MRE under square error loss and it has finite variance, then it is minimax under square error loss. 
\end{theorem}
\subsection{Approximate Minimax}
\begin{theorem}[Le Cam's Two point Lemma]
For any $P_0 , P_1 \in P$
$$\inf_A \sup_P R(A,P) \geq \frac{1}{2} \inf_{a} (\ell(a,P_0) + \ell(a,P_1)) (1-d_{TV} (P_0,P_1)).$$
\end{theorem}


\begin{recipe}[Approx Minimax]
$X_i \simiid P_\theta$. 
\begin{enumerate}
	\item Fix $\theta_0, \theta_1$. Often $\theta_0$ something trivial like $0$
	\item Bound by Le Cam's 2 Point Lemma
	$$\frac{1}{2}  (\ell(a,P_0^n) + \ell(a,P_1^n)) (1-d_{TV} (P_0^n,P_1^n)).$$
	\item Bound tv:
	$$d_{TV} (P^n, Q^n) \leq n d_{TV} (P,Q)$$
	Or use Pinsker's Inequality using KL:
	$$d_{TV} (P^n, Q^n) \leq \sqrt{ n/2 (KL(P\| Q))}$$
	\item Choose $\theta_1$ very close to $\theta_0$ st the infimum term is very small -- gives the minimax order, and the tv distance term is constant order. 
\end{enumerate}
\end{recipe}
\begin{example}[Exponential location family]
$X_i \simiid \theta + Exp(1)$. Q: What is order of $\inf_A \sup_\theta \E (\hat \theta - \theta)^2$, ie minimax risk? 

Follow the steps and get 

$$\text{ minimax risk} \leq \frac{\theta_1^2}{4} (1-n\theta_1) \geq \frac{c}{n^2}$$
If we choose $\theta_1 = 1/(2n)$. 


\end{example}
\newpage
\section{Testing}
Types of errors:
\begin{enumerate}
	\item Type 1: Reject the null when the null is true (restricted to size $\alpha$ size of test)
	\item Type 2: Retain the null when the null is false (given by 1-power)
\end{enumerate}
\subsection{Simple-simple}

\begin{theorem}[Neyman-Pearson (NP) Lemma]
For simple versus simple, a MP test always exists and is given by 

$$\phi(x) = \begin{cases}
	1 \quad p_1(x) >kp_0(x)\\
	\gamma \quad p_1(x) = kp_0(x)\\
	0 \quad p_0(x) <kp_0(x)
\end{cases}$$
where constants determined by level constraint. This is just the likelihood ratio test. First find a $k$, then find the $\gamma$. \\\\ Also note that MP test must have level exactly $\alpha$ unless there exists a test with power $1$ with size $<\alpha$. \\

\begin{enumerate}
	\item Existence of MP test of the above form that is level exactly $\alpha$. Is most powerful.
	\item Sufficiency. If a test is of the above form, it is MP test at level
	\item Necessary. Any MP test is of the above form unless there's a test with size $<\alpha$ with power $1$. 
\end{enumerate}
\end{theorem}
Note that non-uniqueness comes from the $=$ condition-- how to randomize. 
\begin{fact}[NP characterizes MP tests]
The NP is actually an if and only if statement. If MP test, then satisfies the above along with level constraint. MP test must be of this form.
\end{fact}
\subsection{Simple-Composite }
A UMP test only exists if the NP test is the same for \textbf{all} alternatives! If not, have different MP tests, no UMP test! If you want to show that a UMP does not exist, show that the MP test depends on the alternative-- to show exist, does not depend. Eg, testing normal $\theta=0$ versus $\theta\neq 0$, we get different MP tests depending on which side our alternative is on. 
\begin{recipe}[Simple-Composite]
Fix an alternative, find the NP test, argue it doesn't depend on the alternative you chose.
\end{recipe}




 Occurs most often in a one-dimensional parametric family with monotone-likelihood ratio. 
\begin{definition}[Monotone Likelihood Ratio (MLR)]
If exists $T: \cal X \to \R$ statistic that likelihood ratio $p_{\theta'}/p_{\theta_0}$ is \textbf{non-decreasing} in $T$ \textit{whenever $\theta_0$ < $\theta'$. } \\\\
Ie if we have a singleton null and for alternative $\calH_1: \theta'> \theta_0$, then we check whether there's a statistic such that likelihood ratio is increasing.  
\end{definition}


\begin{theorem}[UMP in MLR]
Testing $\theta = \theta_0$ versus $\theta>\theta_0$, if there's a MLR, then the UMP test given by

$$\phi(x) = \Ind[T(X) > C ] + \gamma \Ind[T(X) = C].$$
where we choose $C$ to get $\E_{\theta_0} \phi(X) = \alpha$. \\ \\

Moreover $\beta(\theta)$, the power function, is \textbf{increasing} as a function of $\theta$.  
\end{theorem}
\begin{fact}[MLR in Exponential Families]
Exponential families satisfy MLR! Also satisfied in non-central chi-square.
\end{fact}



\subsection{Composite-Simple}



An important insight is that:

$$\{ \text{tests valid for full composite null} \} \subset \{ \text{tests valid for a specific point in the null}\}.$$
So if you can find a best test for a specific point, and it is also valid for the full null, then it is the best for the composite null. \\\\

If we have a composite null, to find UMP test we put a prior on the null parameter space and reduce to a simple-simple. If the MP test in this simple-simple problem is also \textbf{valid} for the original problem null, then it is the UMP for composite-simple testing problem. Ie:

$$\tilde P_0 (\cdot) = \int_{H_0} P(\cdot) d\Pi(P) \quad \text{weighted average of null measures}$$
\begin{definition}[Least favorable prior]
The prior with the lowest powered MP test, ie $\beta_{\Lambda} \leq \beta_{\Lambda'}$ for all $\Lambda'$ prior on $\Theta_0$. 
\end{definition}

\begin{recipe}[Composite-Simple]

To find UMP test:
\begin{enumerate}
	\item Posit a least favorable prior
	\item Compute the simple-simple MP test $\phi_\Pi$ wrt the simple null from the LFP
	\item Show that $\phi_\Pi$ is \textit{valid} for the original problem, ie $\E_{\theta_0} \phi_\Pi \leq \alpha$ for all $\theta_0 \in \calH_0$.
	\item Conclude that $\phi_\Pi$ is UMP in the original problem
\end{enumerate}
Unique UMP if $\phi_\Pi$ is unique in the simple-simple case.\\\\
For least favorable prior, try to make it hard to reject. Ie, mass on the boundary. 
\end{recipe}

The idea is that for any other valid test will also be valid for the mixture null, but our test $\phi_\Pi$ is most powerful in that family. 
\subsection{Composite-Composite}
UMP rarely exists in composite-composite. To check, always first fix an alternative, find MP test for composite-simple case. Then show that it does or doesn't depend on the alternative chosen. Here are the times when it does:

\begin{enumerate}
	\item One-side MLR testing: $\calH_0: \theta\leq \theta_0$, $\calH_1: \theta>\theta_0$. The test is the same regardless of which alternative we choose. 
	\item Bioequivalence: Test null that $\theta \notin (\theta_1,\theta_2)$. If we have a 1-param expo family, then $\phi(x) = \Ind[T\in (C_1, C_2)]$ is UMP
	\item One sided testing of Gaussian mean, ie $\calH_0: v^T \mu = \delta$ versus $\calH_1: v^T \mu >\delta$ for fixed $v$ with $v^T \Sigma v>0$, then reject for large $v^T X$. 
\end{enumerate}


\subsection{Examples}

\begin{example}[$X\sim N(\mu_x, \sigma^2$, $Y\sim N(\mu_y,\sigma^2)$ with $\sigma$ known]
Want to test $\mu_y\leq \mu_x$ versus not. 
\begin{enumerate}
	\item Fix an alternative $\theta_x, \theta_y$. 
	\item Put a point mass prior on average of the $\mu_x = \mu_y$ line ie $(\theta_x + \theta_y)/2$ for both
	\item Get NP test: $\frac{Y-x}{\sqrt{2} \sigma} > z_{1-\alpha}$
	\item Need to check that level $\alpha for original problem$. So fix a null $\mu_x,\mu_y$ such that $\mu_y\leq \mu_x$. 
	\item Conclude that MP for testing $\calH_0$ versus $(\theta_x, \theta_y)$
	\item Since doesn't depend on the specific alternative chosen, it's UMP
\end{enumerate}
\end{example}
\begin{example}[Non-parametric - hypothesis about the cdf $\calH_0: F_{X_i} (u) \geq p_0$]
Idea is to parametrize $P$ by conditional distributions:
\begin{enumerate}
	\item $p = P(X_i \leq u)$
	\item $P^-$ Distribution of $X_i| X_i \leq u$
	\item $P^+$ Distribution of $X_i| X_i > u$
\end{enumerate}
Least favorable prior will be have the same conditional distributions with a different $p$ such that in the null space. (Point mass). Becomes isomorphic to testing $Bin(n,p)$ $p \geq p_0$. 
\end{example}

Weird: we have a MP but not UMP for testing $H_0: \sigma \geq \sigma_0$ where both $\mu,\sigma$ unknown-- place point mass LFP. By contrast, testing $\sigma \leq \sigma_0$ we can't use a point mass- need a normal prior. Leads to UMP test. $\phi = \Ind[\frac{\sum_{i=1}^n (X_i - \bar X)^2 }{\sigma_0^2} > c_{n-1, 1-\alpha}]$. 

\subsection{P-values}

\begin{definition}[p-value]
Given some nested rejection regions for valid test, then
$$\hat p(X)= \inf \{\alpha: X\in R_\alpha\}.$$
\end{definition}
\begin{fact}[P-values are sub-uniform]
P values are sub-uniform. If we have exact level $\alpha$ for any $\alpha$, then $\hat p$ is exactly uniform.
\end{fact}


\section{UMPU}
What we do when there's no UMP? 
\begin{definition}[Unbiased test]
Unbiased level $\alpha$ if 
\begin{enumerate}
	\item Level $\alpha$: $\E_\theta \phi \leq \alpha \quad \forall \theta\in \Omega_0$
	\item $\E_\theta \phi \geq \alpha \quad \forall \theta \in \Omega_1$.
\end{enumerate}
Think -- for a specific alternative, we never have power worse than $\alpha$.
\end{definition}

Helpful that in exponential families, power function is smooth. This means that if $\beta$ is power function, and simple null $\theta_0$, then $\beta '(\theta_0) = 0$ is a minimizer.

\begin{theorem}[Simple null UMPU, 1-param Expo]
Want to test:

$$\calH_0: \theta= \theta_0, \quad \calH_1: \theta \neq \theta_0.$$

Given by 

$$\phi(X) = \begin{cases}
	1 & T(X) >c_1 \text{ or } T(X) <c_2\\
	\gamma_i & T_i = c_i\\
	0 &\text{ otherwise (between $c_1, c_2$)}
\end{cases}.$$
Where the constants above must satisfy level constraint and also so that:

$$\E_{\theta_0}[T(X) \Phi(T(X)) ] = \alpha \E [T(X)].$$

\end{theorem}
\begin{example}[$X_i \sim N(\theta,1)$]
If want to test $\theta = 0$ in normal location model with known variance, the equitailed test is UMPU; see above.
\end{example}


One unbiased proof argument is to compare to the $\phi = \alpha$ test. Our test above is the best power in the class where $\beta'(\theta_0) = 0$, therefore it must be unbiased, since it beats out the constant test.


\begin{definition}[Similar (on boundary) test]
If $E_\theta \phi = \alpha$ for all $\theta$ on the boundary. \\\\
Unbiased tests are similar if power function is continuous: $\text{ unbiased tests } \subset \text{ similar tests}$.\\\\
We find the UMP among similar tests and then show it's unbiased.
\end{definition}
\begin{definition}[Neyman structure]
Let $T$ be sufficient at the boundary. Then $\phi$ has Neyman structure if $\E [\phi | T=t] = \alpha$ for all $t$. It's like conditional similarity. \\\\

Equivalent to $T$ being complete and sufficient for the boundary if $\phi$ is a similar test. \\\\

If $T$ is CSS, then every similar test has Neyman structure. 
\end{definition}

Idea is $\E_{\theta, \sigma} \phi (X) = \E [\E \phi | T] $, maximize the inner expectation. \\\\

\begin{recipe}[UMPU in multiparameter exponential family]
Todo
\begin{enumerate}
	\item Get nice form
	$$p_{\theta, \xi} (x) = \exp(\theta U(X) + \xi T(X) - \Psi(\theta,\xi)$$
	\item Where parameter $\theta$ that's equivalent to testing original parameter
	\item T is CSS at boundary. Then $U(X)|T$ does not depend on the nuisance $\xi$ on the boundary
	\item Reject for $U > k(T)$ etc depending on side of test
	\item $k$ is found by satisfying $P_\theta (U > k(T) | T) = \alpha$ at boundary. 
	\item Helpful to try to use Basu's theorem 
	\item To satisfy the orthogonality condition, hope to use symmetry.
\end{enumerate}
\end{recipe}

\myworries{Add the recipes here}

\begin{example}[Inference on Winners Example]

\end{example}

\section{Invariant Tests}
Invariance-- looking for transformations of the data that don't change the testing problem at all. What is a map that takes nulls to nulls and alternatives to alternatives? 
\begin{example}[$X_i \sim N(\theta_i, \sigma^2)$ with $\sigma$ known ]
Test $\theta_i=0$ for all $i$ versus not. Eg linear model. An invariance is orthogonal transformation. Applying $UX$ maps nulls to nulls and alternatives to alternatives. 

$$\Phi(X) = \Phi(U X)$$

Test depends only on distance of $\underline X$ from the origin, ie on $T=\sum_i X_i^2$. What is the UMP if we only observe $\sum X_i^2$, instead of our full data?\\\\

UMPI rejects when $\frac{\sum X_i^2}{\sigma^2} > q(1-\alpha)$ of $\chi^2_{n}$.
\end{example}

Invariance reduces the sample space, ie $(X_1, \ldots, X_n) \mapsto T(X)$. \\\\

The general setup is that we have a $g$ that maps $X' = gX$. This induces a transform $\bar g$ that maps $\bar g \theta = \theta'$. We want that $\bar g(\Omega_0)\subset \Omega_0$ and $\bar g(\Omega_1) \subset \Omega_1$. Ie, the map keeps us within classes. Class $G$ of such transformations must be a \textbf{group}, ie having inverse and composition. We say that the problem is "invariant" with respect to $G$: testing based on $X$ or $gX$ is the same. 

Need:
\begin{enumerate}
	\item Applying $g$ we don't leave our model
	\item For any $g$ we pick, for any $\theta$ we pick, there's some $\theta'$ such that $g$ takes $\theta'$ to $\theta$. Ie, $g(X)$ under $P_\theta'$ has distribution $P_\theta$. That is, any $g$ we choose induces a surjection onto $\Theta$. 
	\item Stay within $\calH_0$ or $\calH_1$.Â 
\end{enumerate}

\begin{definition}[Invariant tests]
$$\phi(X) = \phi(gX) \quad \forall g\in G , \forall X\in S.$$
Our test gives the same result regardless of whether we transform. The hope is that there's a UMP test among invariant tests (depends on the group you choose). 
\end{definition}

Look for-- linear regression type problems, unitary matrices, translations. 

\begin{definition}[Maximal Invariant]
Given a group of transformations $G$. Then $T$ is maximal invariant if:
\begin{enumerate}
	\item $T$ invariant ie $T(gX) = T(X)$ for all $g,X$. 
	\item If $T(\underline X_1) = T(\underline X_2)$, there exists a $g$ such that $X_2 = gX_1$. 
\end{enumerate}
Eg, $\sum X_i^2$ is max invariant in the $N(\theta_i,\sigma^2)$ example. \\\\
\textbf{Any invariant function (eg test) is a function of the maximal invariant. So we can consider tests that are only functions of the maximal invariant!!!! TSH 6.2.1}
\end{definition}

\begin{example}[Location Changes]
If $g X = X + c$, then maximal invariant is:

$$n = 1: \quad \text{ constant functions}.$$
$$n>1: \quad T = Y = (Y_1,\ldots, Y_{n-1}) = (X_1 - X_n,\ldots, X_{n-1} - X_n)$$

The proof is that there exists a $c = X_n' - X_n$ such that if $T(X) = T(X')$, there exists a $c = X_n' - X_n$ such that $X ' = X + c$.  \\\\

Alternatively it's sometimes helpful to use $(X_1 - \bar X, \ldots, X_{n-1} - \bar{X})$. 
\end{example}

\begin{example}[Scale Changes]
$g(X) = (cX_1,\ldots, cX_n)$, then $Y= (\frac{X_1}{X_n},\ldots, \frac{X_{n-1}}{X_n})$ is maximal invariant. $c = \frac{X_n'}{X_n}$. 
\end{example}

\begin{example}[Group of increasing functions]
If $g(X) = (f(X_1),\ldots, f(X_n))$ for some increasing $f$, then $g$ preserves the order but not the values. \textbf{Ranks} are maximal invariant. 
\end{example}

\begin{recipe}[Find the UMPI]
\begin{enumerate}
	\item Reduce by sufficient stat if necessary
	\item Find the transform group $G$ that leaves the problem invariant 
	\item Find the maximal invariant
	\item Test based on the maximal invariant (ie find UMP test as if this was the only thing observed.)
\end{enumerate}	
\end{recipe}



%% end 300a
\end{document}