\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{hw}
\usepackage{tcolorbox}
\usepackage[ruled,vlined]{algorithm2e}

% ------------------  Bibliography  ------------------
%\usepackage[
%  backend=biber,
%  style=numeric,      % plain numbers: [1]
%  sorting=none,       % keep refs in citation order
%  maxbibnames=99      % show all authors in bib
%]{biblatex}
\usepackage{booktabs}
% ----------------------------------------------------

\tcbuselibrary{breakable}
\title{300 Quals Guide}
\newcommand{\answer}[1]{%
  \begin{tcolorbox}[
    colback=gray!9.8,
    boxrule=0.5pt,
    breakable]
  \small #1
  \end{tcolorbox}}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\newcommand{\simiid}{\overset{iid}\sim }
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\row}{\operatorname{row}}
\newcommand{\Proj}{\operatorname{Proj}}
\newcommand{\Unif}{\operatorname{Unif}}

\begin{document}
\maketitle
\section{Misc facts}
\begin{definition}[Convexity]
For all $0\leq t\leq 1$ and $x_1,x_2 \in X$:
$$f(tx_1 + (1-t)x_2) \leq tf(x_1) + (1-t) f(x_2).$$
Alternative, check the second derivative $\geq 0$
\end{definition}
\begin{theorem}[Jensen's Inequality]
If $\phi$ convex, then $\E \phi(X) \geq \phi(EX)$. Eg $EX^2 \geq (EX)^2$. Inequality is strict if $\phi$ is strictly convex and $X$ is not degenerate (constant). Also conditional version holds. 
\end{theorem}
\section{Exponential Families}
\begin{definition}[Exponential Family]
	$$p_\theta(x) = \exp(\sum_{i=1}^s T_i(X) \eta_i(\theta) - A(\theta))h(x)$$
	$$p_\eta(x) = \exp(\sum_{i=1}^s T_i(X) \eta_i - \tilde A(\eta)) h(x)$$
\end{definition}
\begin{fact}[$E[T(X)$, $Cov(T(X))$]
$$\E[T(X)] = \grad A(\eta)$$
$$\Cov (T_i(X), T_j(X)) = \partial_{\eta_i} \partial_{\eta_j} A(\eta)$$
\end{fact}

\subsection{All the Expo family examples}

Include curved.. 


\section{Sufficiency} 
"Throwing away everything else besides this statistic entails no loss of info in estimating $\theta$"
\begin{definition}[Sufficiency]
If for all $t$, the distribution of $X|T=t$ does not depend on $\theta$.
\end{definition}
If you collect data $X_1,\ldots,X_n$, then throw away data except for $T=t$, then can construct $\tilde X_1,\ldots \tilde X_n$ with same distribution as $\underline{X}$ just by knowing $T$.

\myworries{Make a table of good examples including $\Unif(0,\theta)$}
\begin{theorem}[Factorization Theorem]
$T$ sufficient for $\theta$ (or for the model) $\iff$ we can write $p_\theta(\underline{x}) = g_\theta(t(\underline{x})) h(\underline{x})$. Ie, can factor the density into a part that depends on statistic and $\theta$, and another part that depends on data but not $\theta$. 
\begin{example}[Expo($\theta$)]
Can write $p_\theta(\underline x) = \theta^n \exp(-\theta\sum x_i)$ so $T(X) = \sum_{i=1}^n X_i$ is sufficient.
\end{example}
\begin{example}[Exponential families]
Look at the form -- $T$ is sufficient.
\end{example}


\end{theorem}
\begin{definition}[Minimal sufficiency]
For all $T'$ sufficient, $T$ is a function of $T'$.
\end{definition}


\begin{theorem}[Rao-Blackwell]
If $T$ sufficient, $L$ is any convex loss, and $\delta$ estimates $g(\theta)$, define:

$$\eta(T) = \E[\delta(X)|T],$$
then using Tower property and Jensen's:

$$R(\theta, \delta(X) = \E[g(\theta), \delta(X)] \geq \E [L(\theta, \eta(T)) = R(\theta, \eta(T))$$
\end{theorem}
\begin{definition}[Ancillary Statistic]
Distribution of $A(X)$ does not depend on $\theta$.
\begin{definition}[First Order Ancillary]
If $\E_\theta A(X)$ does not depend on $\theta$. Weaker than Ancillary. 
\end{definition}
\end{definition}
\section{Completeness} 
\begin{definition}[Completeness]
$T(X)$ complete if no non-constant function of $T$ is even 1st order ancillary, or equivalently:

$$\E f(T) = 0 \implies f(T) = 0 \text{ a.s}.$$
\end{definition}
\begin{example}[$X_i\sim\Unif(0,\theta)$]
See coaching notes-- break $h$ into positive and negative parts to show $h = 0$ as. 	
\end{example}
\begin{theorem}[Full rank exponential family]
Ie if $(\eta_1,\ldots, \eta_k): \eta \in \Omega$ contains a $k-$dimensional rectangle, then $T$ is not only sufficient, but \textbf{also complete}.
\end{theorem}
\begin{theorem}[Basu's Theorem]
If $T$ is CSS and $A$ Ancillary, then $T\indep A$. 
\end{theorem}
A common strategy with Basu's theorem is to consider a submodel-- show independence in the submodel, then based on arbitrariness of submodel, show true for full model. 
\begin{example}[Show sample mean and sample variance in $N(\mu,\sigma^2)$ are independent]
(Assume both unknown). then in the submodel where $\sigma = \sigma_0$ known, then $\bar X$ is CSS. Can show that $\sum (X_i - \bar X)^2$ is Ancillary. So $\bar X \indep \sum (X_i - \bar X)^2$. This is true for all $\mu$, and for $\sigma = \sigma_0$ fixed. But $\sigma_0$ arbtirary, so true for all $\mu,\sigma$. 	
\end{example}
\myworries{Add a table here including 2 param Unif}
\begin{enumerate}
	\item $\Unif(0,\theta)$ : $T= X_{(n)}$
	\item $\Unif(\theta_1, \theta_2)$: $T = (X_{(1)}, X_{(n)})$
\end{enumerate}

\section{UMVU}
Note that we can't take existence of an unbiased estimator for granted. If $x\sim Bin(n,\theta)$. Take like $g(\theta) = \frac{1-\theta}{\theta}$ or a polynomial of degree $>n$-- no unbiased estimators. Strategy to show no unbiased estimator is just write out the expectation of an estimator.
\begin{definition}[UMVU]
$\delta^*$ is unbiased and for all other $\delta$, $R(\delta^*,g(\theta)) \leq R(\delta,g(\theta))$ for all $\theta$.
\end{definition}
Note that if we have an unbiased estimator, we can Rao-Blackwellize with a sufficient statistic and still have unbiased estimator. 
\begin{theorem}[Lehman-Scheffe]
If unbiased estimator is function of CSS, it is UMVU. (Since there exists at most one unbiased estimator that's function of $T$ by completeness).\\\\
Alternate statement: if there exists any unbiased estimator and a CSS $T$, then there is a unique unbiased estimator that's a function of $T$, which is UMVU (UMRU for any convex loss). Unique UMRU if strict convex loss, since this makes Jensen strict. 

\end{theorem}
\begin{recipe}[UMVU with CSS]Steps:
	\begin{enumerate}
	\item Find a CSS
	\item Find an unbiased estimator function of CSS. (Alternatively, find a dumb unbiased estimator and RB)
	\item $\implies$ UMVU
\end{enumerate}
\end{recipe}
Note that UMVU can be inadmissible - see James-Stein or the Poisson UMVU for $g(\theta).= \exp(-3\lambda)$ example in Lecture 5. 
\begin{theorem}[Orthogonality Condition]
Let $\hat \theta$ be an unbiased estimator with $E_P \hat\theta^2 <\infty$ if $U(X)$ is an unbiased estimator of $0$ for all $P$, then:

$$\hat \theta \text { UMVU } \iff E_P [\hat \theta(X) U(X) ] = 0 \text { for all unbiased estimators of } 0 \text{ and for all P}$$
An application is showing that the addition of UMVUs is UMVU. Also, product of UMVUs is UMVU for its expectation. \\
Also reasonable to try to show not UMVU. 
\myworries{Come back to Ex 2.3 in Notes}
\end{theorem}
\begin{theorem}[Cramer Rao LB]
For any $\hat \theta$ unbiased for $\theta$:

$$\Var_\theta \hat \theta \succ I_\theta^{-1}$$
when $\{P_\theta\}$ is QMD with non singular $I_\theta$. 
\end{theorem}

\subsection{UMVU Examples}
\begin{enumerate}
	\item Basic Expo Fam examples -- eg $\bar X$ in Bernouli or Normal with known variance 
	\item Empirical CDF in $\mathcal N(\theta,1)$.
	\begin{itemize}
		\item CSS is $\bar X$, $\delta = \Ind[X_1 < u]$ unbiased. 
		\item Idea to RB then add and subtract $\bar X$ since $X_1 - \bar X$ is ancillary, then apply Basu
		\item UMVU is $\Phi(\frac{u - \bar X}{\sqrt{(n-1)/n}})$
	\end{itemize}

\end{enumerate}
Non parametric examples
\begin{enumerate}
	\item $X_i \sim F \in \calF = \{ \text{ all distributions with density wrt Lebesgue and finite variance } \}$. $g(\theta) = E_F X_i$.
	\begin{itemize}
		\item Note that $\bar X$ is unbiased in the big family and is UMVU in the normal \textbf{subfamily} 
		\item Order statistics CSS (always sufficient - complete by subfamily arg and bijection with sums of powers): $(X_{(1)}, \ldots, X_{(n)}) \iff (\sum X_i, \ldots, \sum X_i^n)$ bijection. 
		\item So $\bar X$ is UMVU
	\end{itemize}
	\item $X_i$ iid symmetric about $\theta$, $E X_i = \theta$. Finite variance. 
	\begin{itemize}
		\item Two subclasses: normal family, $\Unif(\theta_1, \theta_2)$ family - have different UMVUs and both are unbiased in the original class
	\end{itemize}
\end{enumerate}
\begin{recipe}[Subfamily UMVU Argument]
If UMVU in subfamily and unbiased in big family, must be UMVU in big family \textbf{if} an  UMVU exists in the big family. Because the UMVU \textbf{uniquely} minimizes variance in the subfamily. 
\begin{proof}
	Take the big family. Take a function such that $E_\theta f(X) = 0$ for all $\theta$. Then true for subfamily. So $P_\theta (f(X) = 0)$ for all $\theta$ in the subfamily. Same null sets as big family, means that also $P_\theta (f(X) = 0)$ for the big family.
\end{proof}
\end{recipe}
\begin{fact}[Completeness in subfamily]
If $\calF_0 \subset \calF$ and they have the same null sets, then completeness in $\calF_0$ implies completeness in $\calF$. 
\end{fact}
\begin{recipe}[Non-existence of Non-parametric UMVU]
Find two different subclasses with different unique UMVU that are also unbiased in the big class -- no UMVU in big class.	
\end{recipe}
If we can't use Lehman-Scheffe, try one of the following ideas:
\begin{enumerate}
	\item Cramer-Rao lower bound
	\item Orthogonality condition
	\item Subfamily arguments
\end{enumerate}
\subsection{Non-convex loss functions}
If loss is bounded, there is no UMRU estimator. This is unbiased:
$$\delta_\pi = \begin{cases}
	g(\theta_0) \text{ wp } 1-\pi\\
	\frac 1 \pi [\delta_0 (X) - g(\theta_0)] + g(\theta_0)
\end{cases}$$
and its risk is $\pi M$, so it could be arbitrarily small.



\section{MRE}
Big picture-- there's an easy recipe for MRE for square error or absolute error if we pick any old $\delta_0$ equivariant function of CSS. 

\begin{definition}[Location models]
Density satisfies
$$f_{\theta + h} (x+h) = f_\theta(x)$$
Think: normal at its mean has same density as a shifted normal at shifted mean.\\ 
A \textbf{location invariant loss} is $\ell(a + h,\theta + h ) = \ell(a,\theta)$ for all $a, \theta, h$. 
\end{definition}
Want location equivariant estimators, ie $\hat \theta(x + h) = \hat(\theta) + h$ -- if our observations all shift by some amount, our estimate of the location should also shift by that amount. 

\begin{fact}[Bias, variance, risk of equivariant estimators]
Do not depend on $\theta$. Ie, 
$$\E_\theta \hat \theta = \theta + b \text{ for all } \theta $$
for risk, since it's constant, we can hope to find the best among all equivariant estimators.
\end{fact}

\begin{definition}[MRE]
Satisfies equivariance condition: $$\hat \theta(x+c, u) = \hat \theta (x,u) + c$$
and minimum risk condition amongst all equivariant estimators. 
\end{definition}
\begin{definition}[Characterization of location \textbf{invariant} estimators]
Location invariant means $U(X_1 + c,\ldots,X_n + c) = U(X)$. Characterize by:
$U$ location invariant $\iff U = V(y_1, \ldots, y_{n-1})$ where $y_i = X_i - X_n$. 
\end{definition}
\begin{fact}[Characterization of location \textbf{equivariant} estimators]
Let $\delta_0$ be \textbf{any} location equivariant estimator, eg $\delta_0 = \bar X$. 
$$\delta \text { is location equivariant} \iff \delta(X_1,\ldots,X_n) = \delta_0(X_1,\ldots,X_n) + U(X_1,\ldots, X_n) $$ 
where $U$ is location \textbf{invariant}. Then from invariant characterization:
$$\iff \delta(X_1,\ldots,X_n) = \delta_0(X_1,\ldots,X_n) + V(Y_1,\ldots, Y_{n-1})$$
\end{fact}

\begin{recipe}[Finding the MRE]
Let $X_i$ observations iid from location model and let $Y = (X_1 - X_n, \ldots, X_{n-1} - X_n)$. Let $\hat \theta_0$ be any location equivariant estimator of $\theta_0$ with finite risk. If the following is well-defined:
$$v(y) = \arg \min _v E_0 [\ell(\hat \theta_0 (X) - v, 0) \mid Y = y]$$
Then there exists an MRE $\hat \theta^*(X) = \hat \theta_0(X) - v(Y)$.	
\end{recipe}

Want to minimize 
	$$\E_\theta [ \rho (\delta_0(X)- V(Y) - \theta)] = \E_0 \rho (\delta_0(X) - V(Y))$$
	Apply Tower property and minimize the inner expectation. For square error yields:
	$$V = \E_0 [\delta_0 (X) |Y)]$$
	So $\delta^* = \delta_0 - E_0 [\delta_0 |Y]$
Notice that the $Y_i$ here are ancillary so if we pick $\delta_0$ function of CSS, easy. 


If we can make $\delta_0$ a function of CSS, then we can apply Basu
\begin{recipe}[MRE for Square Error Loss]
	Choose $\delta_0$ equivariant function of CSS. Then $\delta^* = \delta_0 - E_0 \delta_0$ is MRE. 
	\begin{example}[$X_i \sim N(\theta,1)$]
	Let $\delta_0 = \bar X$. Then $E_0 \delta_0 = 0$. So $\bar X$ is MRE.
\end{example}
\end{recipe}
Note for absolute error, just do median$_{\theta=0}[\delta(X)|Y]$ instead of expectation. Ie find $m$ such that $P(X\leq m) \geq 1/2$ and $P(X\geq m) \geq 1/2$. 

\begin{theorem}[Existence of MRE]
If loss is convex and not monotone, then MRE exists by the previous theorem. If strictly convex, unique. 
\end{theorem}





\begin{fact}[MRE is Unbiased (under squared error loss)]
Since the bias does not depend on $\theta$, just subtract off whatever bias. 
\end{fact}

\begin{fact}[UMVU is MRE if UMVU is location equivariant]
In a location model, the UMVU is location equivariant. Since MRE is the best amongst equivariant estimators, and any competing equivariant estimators are unbiased..
\myworries{Is this just with square error loss?}

\end{fact}


\begin{theorem}[Anderson's Lemma]
\myworries{Review this}
$Z\sim N(0,\Sigma)$, and $\ell$ is bowl shaped, then $\E \ell (Z) \leq \E \ell (Z+U)$ for any $U\indep Z$. 
\end{theorem}

 \begin{definition}[Pitman Estimator]
\end{definition}
\begin{theorem}[Mini-convolution theorem]
\end{theorem}

\section{James-Stein Estimator}
Setup: $\calN(\mu,\sigma^2)$ with $\sigma^2$ known. 

\begin{theorem}[SURE - Stein Unbiased Risk Estimator ]
Letting $\hat \mu(x) = x+g(x)$ for $g: \R^p \to \R^p$ almost differentiable, and assume that $\E[\sum_{i=1}^p |\partial_i g_i (X)|] <\infty$. Then:

$$\E_\mu[\|\hat \mu(X) - \mu\|^2] = p\sigma^2 + \E\left[\|g(X)\|^2 + 2\sigma^2 \sum_{i=1}^p \partial_i g_i(X)\right].$$
Proved using integration by Parts -- see Lec 17 300c.
\end{theorem}
\begin{fact}[UMVU is not admissible in $\calN(\mu,1)$ model]
Because James Stein renders $X$ inadmissible. 	
\end{fact}

J-S estimator is given by:

$$\hat \mu^{JS} (X) = \left ( 1-\frac{\sigma^2(p-2)}{\|X\|_2^2}\right)X,$$
biased towards the origin. Prove that it has better risk by SURE estimator. 

\section{Bayes Estimators}
\begin{fact}[Unique Bayes is Admissible]
Idea is that if $R(\hat \theta ' ,\theta) \leq R(\hat \theta , \theta)$ for all $\theta$, it would then be Bayes. \\ provided the prior isn't super weird (eg continuous dist with an atom) 

\end{fact}

\begin{fact}[Constant risk Bayes is minimax]
If not, some other estimator would render Bayes inadmissible, which would make that estimator the Bayes estimator.
\end{fact}

\begin{fact}[Bayes is not UMVU if $r_\Lambda <\infty$]
\myworries{Under square error loss, Bayes estimators are biased }
\end{fact}

\section{Minimax}
A Bayes or admissible estimator is constant risk, then it is minimax. 

\end{document}